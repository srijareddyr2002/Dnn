{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oxbYjHUbRN-",
        "outputId": "2663bea2-dee1-4f0a-cd86-e35cd6e0f5a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
            "Requirement already satisfied: scipy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.11.4)\n",
            "Requirement already satisfied: torch in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.8.0+cu128)\n",
            "Requirement already satisfied: torchvision in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.23.0+cu128)\n",
            "Requirement already satisfied: torchaudio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.8.0)\n",
            "Requirement already satisfied: pytorch-lightning in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (2.6.0)\n",
            "Requirement already satisfied: torchmetrics in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (1.7.4)\n",
            "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (1.3.2)\n",
            "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (4.57.3)\n",
            "Requirement already satisfied: tokenizers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.22.2)\n",
            "Requirement already satisfied: nltk in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (3.9.2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy (from -r requirements.txt (line 20))\n",
            "  Downloading spacy-3.8.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: rouge-score in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (0.1.2)\n",
            "Requirement already satisfied: Pillow in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (12.0.0)\n",
            "Collecting opencv-python (from -r requirements.txt (line 25))\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: imageio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (2.37.2)\n",
            "Requirement already satisfied: scikit-image in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.26.0)\n",
            "Collecting lpips (from -r requirements.txt (line 28))\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 31)) (3.8.2)\n",
            "Collecting seaborn (from -r requirements.txt (line 32))\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: tensorboard in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 33)) (2.15.1)\n",
            "Collecting wandb (from -r requirements.txt (line 34))\n",
            "  Downloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 35)) (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements.txt (line 38)) (6.0.3)\n",
            "Collecting colorama (from -r requirements.txt (line 39))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (2025.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch->-r requirements.txt (line 7)) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pytorch-lightning->-r requirements.txt (line 10)) (25.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pytorch-lightning->-r requirements.txt (line 10)) (0.15.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 14)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 14)) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.36.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (2.32.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 17)) (1.2.0)\n",
            "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 19)) (8.3.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading murmurhash-1.0.15-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading cymem-2.0.13-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading preshed-3.0.12-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading thinc-8.3.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading srsly-2.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.4.2 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy->-r requirements.txt (line 20))\n",
            "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 20)) (2.12.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 20)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 20)) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 20)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 17)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 17)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 17)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 17)) (2025.11.12)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 20))\n",
            "  Downloading blis-1.3.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 20))\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 20))\n",
            "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 20))\n",
            "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 20))\n",
            "  Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: absl-py in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge-score->-r requirements.txt (line 21)) (2.3.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge-score->-r requirements.txt (line 21)) (1.17.0)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python (from -r requirements.txt (line 25))\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-image->-r requirements.txt (line 27)) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-image->-r requirements.txt (line 27)) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 31)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 31)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 31)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 31)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 31)) (3.2.5)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 33)) (1.76.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 33)) (2.41.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 33)) (1.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 33)) (3.10)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 33)) (4.23.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 33)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tensorboard->-r requirements.txt (line 33)) (3.1.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 33)) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 33)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 33)) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 33)) (2.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 33)) (0.6.1)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 34))\n",
            "  Downloading gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 34)) (4.5.0)\n",
            "Collecting sentry-sdk>=2.0.0 (from wandb->-r requirements.txt (line 34))\n",
            "  Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (3.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 10)) (1.22.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 34))\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 34))\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 33)) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 33)) (3.0.3)\n",
            "Downloading spacy-3.8.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (33.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.2/33.2 MB\u001b[0m \u001b[31m157.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.13-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (260 kB)\n",
            "Downloading murmurhash-1.0.15-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (134 kB)\n",
            "Downloading preshed-3.0.12-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (874 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m875.0/875.0 kB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m232.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.3.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m254.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
            "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
            "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m237.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Downloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl (22.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m216.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
            "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl (414 kB)\n",
            "Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
            "Installing collected packages: wrapt, wasabi, typer-slim, spacy-loggers, spacy-legacy, smmap, sentry-sdk, opencv-python, murmurhash, cymem, colorama, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, gitdb, seaborn, gitpython, confection, weasel, wandb, thinc, spacy, lpips\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26/26\u001b[0m [lpips]m24/26\u001b[0m [spacy]hon]hon]\n",
            "\u001b[1A\u001b[2KSuccessfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 colorama-0.4.6 confection-0.1.5 cymem-2.0.13 gitdb-4.0.12 gitpython-3.1.46 lpips-0.1.4 murmurhash-1.0.15 opencv-python-4.11.0.86 preshed-3.0.12 seaborn-0.13.2 sentry-sdk-2.48.0 smart-open-7.5.0 smmap-5.0.2 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.21.1 wandb-0.23.1 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5cijZhlbZ88",
        "outputId": "7314390a-9266-471a-de75-7b470d9de4f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n",
            "PyTorch Version: 2.8.0+cu128\n",
            "CUDA Available: True\n",
            "GPU: NVIDIA H200\n",
            "CUDA Version: 12.8\n",
            "LPIPS Available: True\n",
            "Wandb Available: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 2: Import All Packages\n",
        "# ============================================\n",
        "# Run this cell first in Jupyter notebook\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import yaml\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Numerical Computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Image Processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import imageio\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage import io as skio\n",
        "\n",
        "# Deep Learning - PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, densenet121, vgg16\n",
        "\n",
        "# PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# NLP & Text Processing\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Metrics & Evaluation\n",
        "import torchmetrics\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    hamming_loss,\n",
        "    jaccard_score,\n",
        "    average_precision_score\n",
        ")\n",
        "\n",
        "# Image Quality Metrics\n",
        "try:\n",
        "    from lpips import LPIPS\n",
        "    LPIPS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LPIPS_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  LPIPS not available\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "# Logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Try wandb\n",
        "try:\n",
        "    import wandb\n",
        "    WANDB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WANDB_AVAILABLE = False\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"LPIPS Available: {LPIPS_AVAILABLE}\")\n",
        "print(f\"Wandb Available: {WANDB_AVAILABLE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxd0tz2EdbTk",
        "outputId": "b86cf253-e654-4774-b896-560ef96576ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Dataset path: /teamspace/studios/this_studio/vist_kaggle\n",
            "üìÅ Output directory: /teamspace/studios/this_studio/vist_kaggle/srija_outputs\n",
            "\n",
            "üñ•Ô∏è  Device: cuda\n",
            "    GPU: NVIDIA H200\n",
            "    CUDA Version: 12.8\n",
            "    Memory: 150.12 GB\n",
            "\n",
            "üé≤ Random seed set to: 42\n",
            "\n",
            "üè∑Ô∏è  Tag Vocabulary Size: 51\n",
            "    Objects: 19\n",
            "    Actions: 15\n",
            "    Locations: 15\n",
            "\n",
            "============================================================\n",
            "üìã CONFIGURATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "üìä Data:\n",
            "  ‚Ä¢ Sequence length (K): 5\n",
            "  ‚Ä¢ Image size: 224x224\n",
            "  ‚Ä¢ Max text length: 30 words\n",
            "  ‚Ä¢ Vocabulary size: 10000\n",
            "\n",
            "üèóÔ∏è  Model Architecture:\n",
            "  ‚Ä¢ Visual encoder: resnet50\n",
            "  ‚Ä¢ Visual features: 2048-dim\n",
            "  ‚Ä¢ Text encoder: 2-layer BiLSTM\n",
            "  ‚Ä¢ Text hidden: 512-dim\n",
            "  ‚Ä¢ Tag prediction: 51 tags\n",
            "  ‚Ä¢ Tag embedding: 128-dim\n",
            "\n",
            "üéØ Training:\n",
            "  ‚Ä¢ Batch size: 16\n",
            "  ‚Ä¢ Epochs: 10\n",
            "  ‚Ä¢ Learning rate: 0.0001\n",
            "  ‚Ä¢ Optimizer: adam\n",
            "\n",
            "‚öñÔ∏è  Multi-Task Loss Weights:\n",
            "  ‚Ä¢ Œª_image: 1.0\n",
            "  ‚Ä¢ Œª_text: 1.0\n",
            "  ‚Ä¢ Œª_tag: 0.3 (AUXILIARY)\n",
            "\n",
            "üìà Evaluation Metrics:\n",
            "  ‚Ä¢ SSIM: True\n",
            "  ‚Ä¢ LPIPS: True\n",
            "  ‚Ä¢ BLEU: True\n",
            "  ‚Ä¢ ROUGE: True\n",
            "  ‚Ä¢ Perplexity: True\n",
            "\n",
            "üìÅ Paths:\n",
            "  ‚Ä¢ Dataset: /teamspace/studios/this_studio/vist_kaggle\n",
            "  ‚Ä¢ Checkpoints: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints\n",
            "  ‚Ä¢ Results: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/results\n",
            "  ‚Ä¢ Logs: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/logs\n",
            "\n",
            "============================================================\n",
            "‚úÖ Configuration complete! Ready for next cell.\n",
            "============================================================\n",
            "\n",
            "üíæ Configuration saved to: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/config.yaml\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 3: Configuration & Setup\n",
        "# ============================================\n",
        "\n",
        "# -------------------\n",
        "# 2. DATASET PATHS\n",
        "# -------------------\n",
        "# Update this path to your StoryReasoning dataset location\n",
        "DATASET_BASE_PATH = \"/teamspace/studios/this_studio/vist_kaggle\"\n",
        "\n",
        "# Dataset splits\n",
        "TRAIN_JSON = f\"{DATASET_BASE_PATH}/train.story-in-sequence.json\"\n",
        "VAL_JSON = f\"{DATASET_BASE_PATH}/val.story-in-sequence.json\"\n",
        "TEST_JSON = f\"{DATASET_BASE_PATH}/test.story-in-sequence.json\"\n",
        "\n",
        "# Images folder\n",
        "IMAGES_PATH = f\"{DATASET_BASE_PATH}/train_data\"\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = \"/teamspace/studios/this_studio/vist_kaggle/srija_outputs\"\n",
        "CHECKPOINT_DIR = f\"{OUTPUT_DIR}/checkpoints\"\n",
        "RESULTS_DIR = f\"{OUTPUT_DIR}/results\"\n",
        "LOGS_DIR = f\"{OUTPUT_DIR}/logs\"\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Dataset path: {DATASET_BASE_PATH}\")\n",
        "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# -------------------\n",
        "# 3. DEVICE CONFIGURATION\n",
        "# -------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nüñ•Ô∏è  Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"    GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"    CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"    Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# -------------------\n",
        "# 4. RANDOM SEEDS (for reproducibility)\n",
        "# -------------------\n",
        "SEED = 42\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"\\nüé≤ Random seed set to: {SEED}\")\n",
        "\n",
        "# -------------------\n",
        "# 5. HYPERPARAMETERS CONFIGURATION\n",
        "# -------------------\n",
        "\n",
        "config = {\n",
        "    # ============ DATA PARAMETERS ============\n",
        "    \"data\": {\n",
        "        \"num_frames\": 5,  # K frames as input (predict K+1)\n",
        "        \"image_size\": 224,  # ResNet input size\n",
        "        \"max_text_length\": 30,  # Max words per caption\n",
        "        \"vocab_size\": 10000,  # Vocabulary size\n",
        "        \"train_split\": 0.8,\n",
        "        \"val_split\": 0.1,\n",
        "        \"test_split\": 0.1,\n",
        "    },\n",
        "\n",
        "    # ============ MODEL ARCHITECTURE ============\n",
        "    \"model\": {\n",
        "        # Visual Encoder\n",
        "        \"visual_encoder\": \"resnet50\",  # Options: resnet50, densenet121, vgg16\n",
        "        \"visual_feature_dim\": 2048,  # ResNet50 output\n",
        "        \"freeze_visual_encoder\": False,  # Fine-tune last layers\n",
        "\n",
        "        # Text Encoder\n",
        "        \"text_embedding_dim\": 300,  # Word embedding size\n",
        "        \"text_hidden_dim\": 512,  # LSTM hidden size\n",
        "        \"text_num_layers\": 2,  # LSTM layers\n",
        "        \"text_bidirectional\": True,\n",
        "        \"text_encoder_output_dim\": 1024,  # 512*2 for bidirectional\n",
        "\n",
        "        # Fusion Layer\n",
        "        \"fusion_dim\": 1024,  # After concatenation projection\n",
        "        \"fusion_dropout\": 0.3,\n",
        "\n",
        "        # Tag Prediction Head (INNOVATION)\n",
        "        \"tag_vocab_size\": 100,  # Number of possible tags\n",
        "        \"tag_embedding_dim\": 128,  # Tag embedding size\n",
        "        \"tag_hidden_dim\": 512,  # Tag prediction hidden layer\n",
        "        \"tag_dropout\": 0.4,\n",
        "\n",
        "        # Sequence Model\n",
        "        \"sequence_hidden_dim\": 1024,\n",
        "        \"sequence_num_layers\": 2,\n",
        "        \"sequence_dropout\": 0.3,\n",
        "\n",
        "        # Attention Mechanism\n",
        "        \"attention_dim\": 1024,\n",
        "        \"attention_heads\": 8,  # Multi-head attention\n",
        "\n",
        "        # Image Decoder\n",
        "        \"image_decoder_hidden_dims\": [1024, 2048, 2048],\n",
        "        \"image_decoder_dropout\": 0.3,\n",
        "\n",
        "        # Text Decoder\n",
        "        \"text_decoder_hidden_dim\": 1024,\n",
        "        \"text_decoder_num_layers\": 2,\n",
        "        \"text_decoder_dropout\": 0.3,\n",
        "    },\n",
        "\n",
        "    # ============ TRAINING PARAMETERS ============\n",
        "    \"training\": {\n",
        "        \"batch_size\": 16,  # Adjust based on GPU memory\n",
        "        \"num_epochs\": 10,\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"weight_decay\": 1e-5,  # L2 regularization\n",
        "        \"grad_clip_norm\": 1.0,  # Gradient clipping\n",
        "\n",
        "        # Loss weights (MULTI-TASK LEARNING)\n",
        "        \"lambda_image\": 1.0,  # Image prediction loss weight\n",
        "        \"lambda_text\": 1.0,   # Text prediction loss weight\n",
        "        \"lambda_tag\": 0.3,    # Tag prediction loss weight (AUXILIARY TASK)\n",
        "\n",
        "        # Optimizer\n",
        "        \"optimizer\": \"adam\",  # Options: adam, adamw, sgd\n",
        "        \"betas\": (0.9, 0.999),\n",
        "        \"eps\": 1e-8,\n",
        "\n",
        "        # Learning Rate Scheduler\n",
        "        \"scheduler\": \"reduce_on_plateau\",  # Options: reduce_on_plateau, cosine, step\n",
        "        \"scheduler_patience\": 5,\n",
        "        \"scheduler_factor\": 0.5,\n",
        "        \"scheduler_min_lr\": 1e-7,\n",
        "\n",
        "        # Early Stopping\n",
        "        \"early_stopping\": True,\n",
        "        \"early_stopping_patience\": 10,\n",
        "\n",
        "        # Checkpointing\n",
        "        \"save_every_n_epochs\": 5,\n",
        "        \"save_best_only\": True,\n",
        "    },\n",
        "\n",
        "    # ============ DATA AUGMENTATION ============\n",
        "    \"augmentation\": {\n",
        "        \"horizontal_flip\": True,\n",
        "        \"random_crop\": True,\n",
        "        \"color_jitter\": True,\n",
        "        \"rotation\": 5,  # degrees\n",
        "    },\n",
        "\n",
        "    # ============ EVALUATION PARAMETERS ============\n",
        "    \"evaluation\": {\n",
        "        # Image metrics\n",
        "        \"use_ssim\": True,\n",
        "        \"use_lpips\": LPIPS_AVAILABLE,\n",
        "        \"use_psnr\": True,\n",
        "\n",
        "        # Text metrics\n",
        "        \"use_bleu\": True,\n",
        "        \"use_rouge\": True,\n",
        "        \"use_perplexity\": True,\n",
        "\n",
        "        # Tag metrics\n",
        "        \"tag_threshold\": 0.5,  # Probability threshold for tag prediction\n",
        "    },\n",
        "\n",
        "    # ============ LOGGING & VISUALIZATION ============\n",
        "    \"logging\": {\n",
        "        \"log_every_n_steps\": 50,\n",
        "        \"visualize_every_n_epochs\": 5,\n",
        "        \"num_samples_to_visualize\": 4,\n",
        "        \"use_tensorboard\": True,\n",
        "        \"use_wandb\": WANDB_AVAILABLE,\n",
        "        \"wandb_project\": \"srija-visual-storytelling\",\n",
        "        \"wandb_entity\": None,  # Your wandb username (optional)\n",
        "    },\n",
        "\n",
        "    # ============ TAG CATEGORIES ============\n",
        "    \"tags\": {\n",
        "        \"objects\": [\n",
        "            \"person\", \"dog\", \"cat\", \"car\", \"table\", \"chair\", \"cup\",\n",
        "            \"bottle\", \"tree\", \"building\", \"phone\", \"laptop\", \"book\",\n",
        "            \"bed\", \"window\", \"door\", \"plant\", \"bicycle\", \"motorcycle\"\n",
        "        ],\n",
        "        \"actions\": [\n",
        "            \"walking\", \"running\", \"sitting\", \"standing\", \"eating\",\n",
        "            \"drinking\", \"reading\", \"writing\", \"talking\", \"smiling\",\n",
        "            \"jumping\", \"sleeping\", \"cooking\", \"driving\", \"dancing\"\n",
        "        ],\n",
        "        \"locations\": [\n",
        "            \"kitchen\", \"bedroom\", \"living_room\", \"office\", \"street\",\n",
        "            \"park\", \"restaurant\", \"beach\", \"forest\", \"city\", \"home\",\n",
        "            \"school\", \"hospital\", \"store\", \"garden\"\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    # ============ EXPERIMENT TRACKING ============\n",
        "    \"experiment\": {\n",
        "        \"name\": \"srija_multitask_baseline\",\n",
        "        \"description\": \"Multi-task learning with tag prediction for visual storytelling\",\n",
        "        \"tags\": [\"multi-task\", \"visual-storytelling\", \"tag-prediction\"],\n",
        "        \"notes\": \"Baseline experiment with lambda_tag=0.3\",\n",
        "    }\n",
        "}\n",
        "\n",
        "# -------------------\n",
        "# 6. TAG VOCABULARY SETUP\n",
        "# -------------------\n",
        "# Flatten all tag categories into single vocabulary\n",
        "all_tags = (\n",
        "    config[\"tags\"][\"objects\"] +\n",
        "    config[\"tags\"][\"actions\"] +\n",
        "    config[\"tags\"][\"locations\"]\n",
        ")\n",
        "\n",
        "# Add special tokens\n",
        "TAG_VOCAB = [\"<PAD>\", \"<UNK>\"] + all_tags\n",
        "config[\"model\"][\"tag_vocab_size\"] = len(TAG_VOCAB)\n",
        "\n",
        "# Create tag to index mapping\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(TAG_VOCAB)}\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "\n",
        "print(f\"\\nüè∑Ô∏è  Tag Vocabulary Size: {len(TAG_VOCAB)}\")\n",
        "print(f\"    Objects: {len(config['tags']['objects'])}\")\n",
        "print(f\"    Actions: {len(config['tags']['actions'])}\")\n",
        "print(f\"    Locations: {len(config['tags']['locations'])}\")\n",
        "\n",
        "# -------------------\n",
        "# 7. PRINT CONFIGURATION SUMMARY\n",
        "# -------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Data:\")\n",
        "print(f\"  ‚Ä¢ Sequence length (K): {config['data']['num_frames']}\")\n",
        "print(f\"  ‚Ä¢ Image size: {config['data']['image_size']}x{config['data']['image_size']}\")\n",
        "print(f\"  ‚Ä¢ Max text length: {config['data']['max_text_length']} words\")\n",
        "print(f\"  ‚Ä¢ Vocabulary size: {config['data']['vocab_size']}\")\n",
        "\n",
        "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
        "print(f\"  ‚Ä¢ Visual encoder: {config['model']['visual_encoder']}\")\n",
        "print(f\"  ‚Ä¢ Visual features: {config['model']['visual_feature_dim']}-dim\")\n",
        "print(f\"  ‚Ä¢ Text encoder: {config['model']['text_num_layers']}-layer {'Bi' if config['model']['text_bidirectional'] else ''}LSTM\")\n",
        "print(f\"  ‚Ä¢ Text hidden: {config['model']['text_hidden_dim']}-dim\")\n",
        "print(f\"  ‚Ä¢ Tag prediction: {config['model']['tag_vocab_size']} tags\")\n",
        "print(f\"  ‚Ä¢ Tag embedding: {config['model']['tag_embedding_dim']}-dim\")\n",
        "\n",
        "print(f\"\\nüéØ Training:\")\n",
        "print(f\"  ‚Ä¢ Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"  ‚Ä¢ Epochs: {config['training']['num_epochs']}\")\n",
        "print(f\"  ‚Ä¢ Learning rate: {config['training']['learning_rate']}\")\n",
        "print(f\"  ‚Ä¢ Optimizer: {config['training']['optimizer']}\")\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è  Multi-Task Loss Weights:\")\n",
        "print(f\"  ‚Ä¢ Œª_image: {config['training']['lambda_image']}\")\n",
        "print(f\"  ‚Ä¢ Œª_text: {config['training']['lambda_text']}\")\n",
        "print(f\"  ‚Ä¢ Œª_tag: {config['training']['lambda_tag']} (AUXILIARY)\")\n",
        "\n",
        "print(f\"\\nüìà Evaluation Metrics:\")\n",
        "print(f\"  ‚Ä¢ SSIM: {config['evaluation']['use_ssim']}\")\n",
        "print(f\"  ‚Ä¢ LPIPS: {config['evaluation']['use_lpips']}\")\n",
        "print(f\"  ‚Ä¢ BLEU: {config['evaluation']['use_bleu']}\")\n",
        "print(f\"  ‚Ä¢ ROUGE: {config['evaluation']['use_rouge']}\")\n",
        "print(f\"  ‚Ä¢ Perplexity: {config['evaluation']['use_perplexity']}\")\n",
        "\n",
        "print(f\"\\nüìÅ Paths:\")\n",
        "print(f\"  ‚Ä¢ Dataset: {DATASET_BASE_PATH}\")\n",
        "print(f\"  ‚Ä¢ Checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\"  ‚Ä¢ Results: {RESULTS_DIR}\")\n",
        "print(f\"  ‚Ä¢ Logs: {LOGS_DIR}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Configuration complete! Ready for next cell.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# -------------------\n",
        "# 8. SAVE CONFIGURATION\n",
        "# -------------------\n",
        "config_save_path = f\"{OUTPUT_DIR}/config.yaml\"\n",
        "with open(config_save_path, 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "print(f\"\\nüíæ Configuration saved to: {config_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSUIasufcwBR",
        "outputId": "87209957-2f7c-4228-b32a-8370c66a33dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded TRAIN JSON\n",
            "Top-level keys: ['images', 'info', 'albums', 'type', 'annotations']\n",
            "‚úÖ Built photo_to_caption with ordering\n",
            "   Total photo captions: 64934\n",
            "‚úÖ Built imageid_to_url\n",
            "   Total images: 167528\n",
            "\n",
            "‚úÖ Grouped & sorted stories by frame_order\n",
            "   Total unique story_id: 16021\n",
            "   Stories with ‚â• 5 frames: 8831\n",
            "\n",
            "====================================================================================================\n",
            "üìö TOP 5 COMPLETE STORIES (WITH PROPER FRAME ORDERING)\n",
            "====================================================================================================\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "STORY #1: story_id = 10004\n",
            "Total frames: 5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "  Frame 1 (order=0):\n",
            "    ID:   5574199804\n",
            "    Text: we got stuck in traffic for a long time ....\n",
            "    URL:  https://farm6.staticflickr.com/5150/5574199804_c5cf7712d6_o.jpg...\n",
            "\n",
            "  Frame 2 (order=1):\n",
            "    ID:   5573613249\n",
            "    Text: the cars were stopped for as far as i could see ....\n",
            "    URL:  https://farm6.staticflickr.com/5221/5573613249_19460c3b9e_o.jpg...\n",
            "\n",
            "  Frame 3 (order=2):\n",
            "    ID:   5574199212\n",
            "    Text: so i got out of the car to stretch my legs ....\n",
            "    URL:  https://farm6.staticflickr.com/5182/5574199212_0099cf3ff2_o.jpg...\n",
            "\n",
            "  Frame 4 (order=3):\n",
            "    ID:   5574199262\n",
            "    Text: then i sat on the road to show how nothing was moving ....\n",
            "    URL:  https://farm6.staticflickr.com/5104/5574199262_93ede4451d_o.jpg...\n",
            "\n",
            "  Frame 5 (order=4):\n",
            "    ID:   5574199566\n",
            "    Text: it was even safe to lay among the cars ....\n",
            "    URL:  https://farm6.staticflickr.com/5298/5574199566_8c98d028aa_o.jpg...\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "STORY #2: story_id = 10009\n",
            "Total frames: 5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "  Frame 1 (order=0):\n",
            "    ID:   5580533483\n",
            "    Text: i bet you are wondering how i crashed my brand new car ....\n",
            "    URL:  https://farm6.staticflickr.com/5302/5580533483_d81c3bcb37_o.jpg...\n",
            "\n",
            "  Frame 2 (order=1):\n",
            "    ID:   5580538871\n",
            "    Text: as you can see , it got quite a lot of damage ....\n",
            "    URL:  https://farm6.staticflickr.com/5182/5580538871_ecdb865b2c_o.jpg...\n",
            "\n",
            "  Frame 3 (order=2):\n",
            "    ID:   5580533207\n",
            "    Text: well , the gps caused the accident . i was following it when it said to take a r...\n",
            "    URL:  https://farm6.staticflickr.com/5094/5580533207_3f35451a98_o.png...\n",
            "\n",
            "  Frame 4 (order=3):\n",
            "    ID:   5581123306\n",
            "    Text: the tow truck had to haul me away on a flatbed ....\n",
            "    URL:  https://farm6.staticflickr.com/5226/5581123306_6e3c9a63c6_o.jpg...\n",
            "\n",
            "  Frame 5 (order=4):\n",
            "    ID:   5581125386\n",
            "    Text: i hated seeing my precious car all tied down on that truck !...\n",
            "    URL:  https://farm6.staticflickr.com/5172/5581125386_0ae21475a5_o.jpg...\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "STORY #3: story_id = 10014\n",
            "Total frames: 5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "  Frame 1 (order=0):\n",
            "    ID:   5575313323\n",
            "    Text: my sister came home drunk last night . she drove the car onto the lawn ....\n",
            "    URL:  https://farm6.staticflickr.com/5223/5575313323_35bdd23c8c_o.jpg...\n",
            "\n",
            "  Frame 2 (order=1):\n",
            "    ID:   5575312043\n",
            "    Text: she took out a little tree as well ....\n",
            "    URL:  https://farm6.staticflickr.com/5305/5575312043_075153f45b_o.jpg...\n",
            "\n",
            "  Frame 3 (order=2):\n",
            "    ID:   5575313007\n",
            "    Text: here is the back view of the car that was on our lawn ....\n",
            "    URL:  https://farm6.staticflickr.com/5018/5575313007_5a18aa6394_o.jpg...\n",
            "\n",
            "  Frame 4 (order=3):\n",
            "    ID:   5575901678\n",
            "    Text: the damage the car made can be seen here ....\n",
            "    URL:  https://farm6.staticflickr.com/5256/5575901678_82d8f759f1_o.jpg...\n",
            "\n",
            "  Frame 5 (order=4):\n",
            "    ID:   5575315175\n",
            "    Text: here is another picture of the damage my sister made , but the car was moved bac...\n",
            "    URL:  https://farm6.staticflickr.com/5254/5575315175_02dd31b4b4_o.jpg...\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "STORY #4: story_id = 10019\n",
            "Total frames: 5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "  Frame 1 (order=0):\n",
            "    ID:   5794856113\n",
            "    Text: today marks the opening of the location location location location ....\n",
            "    URL:  https://farm4.staticflickr.com/3023/5794856113_79cfb3b070_o.jpg...\n",
            "\n",
            "  Frame 2 (order=1):\n",
            "    ID:   5794779547\n",
            "    Text: this museum features signed pictures of the states historical figures ....\n",
            "    URL:  https://farm4.staticflickr.com/3310/5794779547_f4e0af0423_o.jpg...\n",
            "\n",
            "  Frame 3 (order=2):\n",
            "    ID:   5795415400\n",
            "    Text: many memories of iconic blues bands are also featured on the walls of the museum...\n",
            "    URL:  https://farm3.staticflickr.com/2248/5795415400_60db87ff66_o.jpg...\n",
            "\n",
            "  Frame 4 (order=3):\n",
            "    ID:   5795416346\n",
            "    Text: there is even a famous guitar hung for display ....\n",
            "    URL:  https://farm6.staticflickr.com/5148/5795416346_7f0762be44_o.jpg...\n",
            "\n",
            "  Frame 5 (order=4):\n",
            "    ID:   5795420678\n",
            "    Text: as well as many records from popular blues bands ....\n",
            "    URL:  https://farm4.staticflickr.com/3607/5795420678_81511cca45_o.jpg...\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "STORY #5: story_id = 10024\n",
            "Total frames: 5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "  Frame 1 (order=0):\n",
            "    ID:   5983721391\n",
            "    Text: today we were on a bus ride and stopped at this beach ....\n",
            "    URL:  https://farm7.staticflickr.com/6022/5983721391_89f8611790_o.jpg...\n",
            "\n",
            "  Frame 2 (order=1):\n",
            "    ID:   5984286876\n",
            "    Text: there were not many people here so that made it nice ....\n",
            "    URL:  https://farm7.staticflickr.com/6010/5984286876_3e5d38c497_o.jpg...\n",
            "\n",
            "  Frame 3 (order=2):\n",
            "    ID:   5984296200\n",
            "    Text: we found some little huts and other things that looked nice ....\n",
            "    URL:  https://farm7.staticflickr.com/6142/5984296200_3c02954191_o.jpg...\n",
            "\n",
            "  Frame 4 (order=3):\n",
            "    ID:   5984314980\n",
            "    Text: there was a boat out in the water signifying that someone else was coming to enj...\n",
            "    URL:  https://farm7.staticflickr.com/6122/5984314980_b98610d2c1_o.jpg...\n",
            "\n",
            "  Frame 5 (order=4):\n",
            "    ID:   5983760397\n",
            "    Text: the sunset was amazing ....\n",
            "    URL:  https://farm7.staticflickr.com/6030/5983760397_83935fa112_o.jpg...\n",
            "\n",
            "====================================================================================================\n",
            "üìä DATASET STATISTICS\n",
            "====================================================================================================\n",
            "\n",
            "‚úÖ Stories with ‚â•5 frames: 8831\n",
            "   Min frames per story: 5\n",
            "   Max frames per story: 5\n",
            "   Avg frames per story: 5.00\n",
            "\n",
            "‚úÖ Saved STORIES dict to memory\n",
            "   Variable name: STORIES\n",
            "   Access: STORIES[story_id] -> list of frames with frame_order, id, url_o, text\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 4 (CORRECTED): Load & Order Stories\n",
        "# ============================================\n",
        "\n",
        "import json\n",
        "from pprint import pprint\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---- Load train.json ----\n",
        "with open(TRAIN_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "print(\"‚úÖ Loaded TRAIN JSON\")\n",
        "print(\"Top-level keys:\", list(train_data.keys()))\n",
        "\n",
        "# =========================================================\n",
        "# PART A: Build lookup tables with ORDERING INFO\n",
        "# =========================================================\n",
        "\n",
        "# annotations[i][0] has: story_id, photo_flickr_id, worker_arranged_photo_order, text\n",
        "# We need to keep worker_arranged_photo_order to order frames correctly\n",
        "\n",
        "photo_to_caption = {}\n",
        "for ann_list in train_data[\"annotations\"]:\n",
        "    if not ann_list:\n",
        "        continue\n",
        "    ann = ann_list[0]\n",
        "    pid = str(ann.get(\"photo_flickr_id\", \"\"))\n",
        "    if pid:\n",
        "        photo_to_caption[pid] = {\n",
        "            \"story_id\": str(ann.get(\"story_id\", \"\")),\n",
        "            \"text\": ann.get(\"text\", \"\"),\n",
        "            \"photo_order\": int(ann.get(\"worker_arranged_photo_order\", -1))  # FRAME ORDER\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Built photo_to_caption with ordering\")\n",
        "print(f\"   Total photo captions: {len(photo_to_caption)}\")\n",
        "\n",
        "# image id -> url_o\n",
        "imageid_to_url = {}\n",
        "for im in train_data[\"images\"]:\n",
        "    iid = str(im.get(\"id\", \"\"))\n",
        "    if iid:\n",
        "        imageid_to_url[iid] = im.get(\"url_o\", \"\")\n",
        "\n",
        "print(\"‚úÖ Built imageid_to_url\")\n",
        "print(f\"   Total images: {len(imageid_to_url)}\")\n",
        "\n",
        "# =========================================================\n",
        "# PART B: Group frames by story_id AND SORT BY ORDER\n",
        "# =========================================================\n",
        "\n",
        "stories_dict = defaultdict(list)\n",
        "\n",
        "for im in train_data[\"images\"]:\n",
        "    iid = str(im.get(\"id\", \"\"))\n",
        "    if not iid:\n",
        "        continue\n",
        "\n",
        "    cap = photo_to_caption.get(iid)\n",
        "    if cap is None:\n",
        "        continue\n",
        "\n",
        "    url = im.get(\"url_o\", \"\")\n",
        "\n",
        "    # Add frame to story with ALL info\n",
        "    stories_dict[cap[\"story_id\"]].append({\n",
        "        \"frame_order\": cap[\"photo_order\"],  # Position in story sequence\n",
        "        \"id\": iid,\n",
        "        \"url_o\": url,\n",
        "        \"text\": cap[\"text\"]\n",
        "    })\n",
        "\n",
        "# SORT EACH STORY BY frame_order\n",
        "for story_id in stories_dict:\n",
        "    stories_dict[story_id].sort(key=lambda x: x[\"frame_order\"])\n",
        "\n",
        "print(\"\\n‚úÖ Grouped & sorted stories by frame_order\")\n",
        "print(f\"   Total unique story_id: {len(stories_dict)}\")\n",
        "\n",
        "# Filter stories: keep only those with ‚â• 5 frames (for K=5 input)\n",
        "min_frames = 5\n",
        "filtered_stories = {\n",
        "    sid: frames for sid, frames in stories_dict.items()\n",
        "    if len(frames) >= min_frames\n",
        "}\n",
        "\n",
        "print(f\"   Stories with ‚â• {min_frames} frames: {len(filtered_stories)}\")\n",
        "\n",
        "# =========================================================\n",
        "# PART C: Display TOP 5 COMPLETE STORIES\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üìö TOP 5 COMPLETE STORIES (WITH PROPER FRAME ORDERING)\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "story_ids_sorted = sorted(filtered_stories.keys())[:5]\n",
        "\n",
        "for idx, story_id in enumerate(story_ids_sorted, 1):\n",
        "    frames = filtered_stories[story_id]\n",
        "    print(f\"\\n{'‚îÄ'*100}\")\n",
        "    print(f\"STORY #{idx}: story_id = {story_id}\")\n",
        "    print(f\"Total frames: {len(frames)}\")\n",
        "    print(f\"{'‚îÄ'*100}\")\n",
        "\n",
        "    for frame_idx, frame in enumerate(frames, 1):\n",
        "        print(f\"\\n  Frame {frame_idx} (order={frame['frame_order']}):\")\n",
        "        print(f\"    ID:   {frame['id']}\")\n",
        "        print(f\"    Text: {frame['text'][:80]}...\")\n",
        "        print(f\"    URL:  {frame['url_o'][:70]}...\")\n",
        "\n",
        "# =========================================================\n",
        "# PART D: Statistics\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üìä DATASET STATISTICS\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "frame_counts = [len(f) for f in filtered_stories.values()]\n",
        "print(f\"\\n‚úÖ Stories with ‚â•{min_frames} frames: {len(filtered_stories)}\")\n",
        "print(f\"   Min frames per story: {min(frame_counts)}\")\n",
        "print(f\"   Max frames per story: {max(frame_counts)}\")\n",
        "print(f\"   Avg frames per story: {sum(frame_counts)/len(frame_counts):.2f}\")\n",
        "\n",
        "# =========================================================\n",
        "# PART E: Save for next cells\n",
        "# =========================================================\n",
        "\n",
        "# Store in global variables for use in next cells\n",
        "STORIES = filtered_stories  # dict[story_id] -> list of ordered frames\n",
        "ALL_STORY_IDS = list(filtered_stories.keys())\n",
        "\n",
        "print(f\"\\n‚úÖ Saved STORIES dict to memory\")\n",
        "print(f\"   Variable name: STORIES\")\n",
        "print(f\"   Access: STORIES[story_id] -> list of frames with frame_order, id, url_o, text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjbWBz6LvPW8",
        "outputId": "a5447d77-e3a2-4416-cfa7-443aaac3312c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Building tokenizer from 44155 texts...\n",
            "üî® Building vocabulary...\n",
            "‚úÖ Vocabulary built: 10000 tokens\n",
            "üíæ Tokenizer saved to: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/tokenizer.pkl\n",
            "\n",
            "üìä Data split (BEFORE subset):\n",
            "   Train stories: 7064\n",
            "   Val stories: 883\n",
            "   Test stories: 884\n",
            "\n",
            "üîß Creating full datasets...\n",
            "   Found 7064 stories with ‚â•5 frames\n",
            "   Found 883 stories with ‚â•5 frames\n",
            "   Found 884 stories with ‚â•5 frames\n",
            "\n",
            "‚ö†Ô∏è  USING SUBSET FOR TESTING\n",
            "   Original train: 7064\n",
            "   Subset size: 200\n",
            "   New train: 200\n",
            "   New val: 40\n",
            "   New test: 40\n",
            "\n",
            "‚úÖ DataLoaders created:\n",
            "   Train batches: 13\n",
            "   Val batches: 3\n",
            "   Test batches: 3\n",
            "   Batch size: 16\n",
            "\n",
            "‚è±Ô∏è  Estimated training time:\n",
            "   Per epoch: ~1.1 minutes\n",
            "   Total (10 epochs): ~10.8 minutes\n",
            "\n",
            "================================================================================\n",
            "üß™ Testing one batch from train_loader\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Successfully loaded batch!\n",
            "üì¶ Batch keys: ['story_id', 'input_images', 'input_texts', 'input_tokens', 'target_image', 'target_text', 'target_tokens']\n",
            "   input_images shape: torch.Size([16, 5, 3, 224, 224])\n",
            "   input_tokens shape: torch.Size([16, 5, 30])\n",
            "   target_image shape: torch.Size([16, 3, 224, 224])\n",
            "   target_tokens shape: torch.Size([16, 30])\n",
            "\n",
            "üìù Sample input texts (first sequence):\n",
            "   Frame 1: ['my family decided that we should go check out medieval times while we were in the city . they gave us each a crown to wear during the show .', 'it was my first night out in awhile and i was excited .', 'waiting at the top for a pleasure boat ride .', 'we hung out to show off new tricks that we learned on our bikes .', 'today was the first day of our trip to location .', 'fran was not very enthused about marrying [male] .', 'there was lots of food in the kitchen', 'the dancers really put on a great performance for the event .', '[female] dressed up in costume for the casino . she was ready to have fun .', 'every sunday , the family gets together for dinner .', '[female] sat at her desk , talking to her cat laying on the ground .', 'my nephew and his bride went to location for their honeymoon .', 'with heavy hearts we marked where the funeral was being held .', 'eating out and trying to eat with chopsticks for the first time .', 'the news broadcasted images from location .', 'as the group hiked across the rolling hills they came across a great deal of graffiti over the natural environment .']...\n",
            "   Frame 2: ['the yellow knight and the blue knight were picked first to battle .', 'some of my friends showed up to keep me company .', 'more people come across the bridge for a boat ride .', 'sometimes we had to wait awhile for the group leader to arrive .', 'we started the day off with some sight seeing .', 'as the vows were spoken , the couple looked disinterested .', 'before they went into the living room .', 'we got an opportunity to meet some really important people .', 'grandma was so happy that we invited her too .', 'this week , grandma was in charge of cooking .', \"`` pepper , what are you doing ? ''\", 'the night they arrived , they watched the sunset .', 'the obituary was respectful and sobering .', 'amazed by the taste of an egg roll .', 'two blimps pass through the city of location promoting the world cup .', 'this time of year , even though the air was drying than normal there was still serenity to be found .']...\n",
            "   Frame 3: ['then the red knight had to battle with the yellow knight .', 'we had a lot of fun talking .', 'how lovely to be taking a boat ride .', 'when he finally arrives it is time to head off .', 'the kids , of course , wanted to go on the rides at the theme park .', 'the time came for [male] to give fran the ring . she was unimpressed .', 'the family ate', \"we had a great turn out for this event . it 's good to see everyone mingling .\", 'we saw a friendly man when we were walking around .', 'she made cornish hens for everyone .', \"`` you playing over there ? ''\", '[male] is a clown and acts like a child at the beach .', 'all her family was gathered , sad but glad to be together in her memory .', 'favorite dish of the night .', 'a crowd rallied on the streets of location to show support for their soccer team .', 'by midday the area fell into a sort of calm near the lake and many people enjoyed the tranquility it brought .']...\n",
            "   Frame 4: ['the red knight had the upper hand . he was our favorite . go red knight !', \"there were also a lot of new people i had n't met before .\", 'the guard in his office monitoring the grounds and water .', \"we ride quickly to our secret location , where authorities wo n't bother us .\", 'we stopped for lunch and the food was delicious !', 'the groomsmen tried to make the occasion more lively .', 'and laughed', \"it 's really encouraging to see that everyone got together to show their support .\", 'we were having drinks and having a lot of fun at the casino . i hope we can go back soon .', 'after eating , they gathered in the living room to share stories .', 'her husband looked over at her , laughing at [female] talking to the cat .', 'the newlyweds buried themselves in the sand and watched the ocean .', 'there were many gorgeous flowers adorning her grave .', 'favorite drink of the night .', 'the crowd cheered for their countries team in the world cup .', 'the boathouse near the lake bustled with activity this time of day .']...\n",
            "   Frame 5: [\"we took this picture on the way out . my little sister had so much fun she did n't want to take her crown off .\", 'it was fun laughing with them .', 'boat riding on a petty summer day .', 'sometimes there are many members ready to show off their skills .', 'we even took a ferry across the inlet .', 'the couple only became happy after cutting the wedding cake .', 'and enjoyed the holiday in the snow .', \"we had the chance to meet some really great people . it 's always great to see people come together for a good cause .\", 'this was the sign of the casino as we were leaving . we were sad to go because we had such a good time .', 'the adults also helped the kids put together a lego kit .', \"[female] 's sister called from the kitchen `` you glad you married this crazy ? ''\", 'after all that sun , the fair-skinned bride had a bit of sunburn so they spent the afternoon in the hotel .', 'and a plaque commemorating her life .', 'walking back home after dinner .', 'the people screamed as their team scored a goal in a match against location .', 'the overlapping , beautiful trees made a walk in the park that much more enjoyable .']...\n",
            "\n",
            "üéØ Sample target text:\n",
            "   we took this picture on the way out . my little sister had so much fun...\n",
            "\n",
            "‚úÖ Dataset loading successful!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 5 (COMPLETE): Custom Dataset Class + Subset\n",
        "# ============================================\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART A: SIMPLE TOKENIZER\n",
        "# =========================================================\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    \"\"\"Basic word-level tokenizer with vocabulary\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=10000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
        "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
        "        self.word_freq = {}\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
        "        print(\"üî® Building vocabulary...\")\n",
        "\n",
        "        for text in texts:\n",
        "            words = text.lower().split()\n",
        "            for word in words:\n",
        "                self.word_freq[word] = self.word_freq.get(word, 0) + 1\n",
        "\n",
        "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for idx, (word, freq) in enumerate(sorted_words[:self.vocab_size-4], 4):\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "        print(f\"‚úÖ Vocabulary built: {len(self.word2idx)} tokens\")\n",
        "        return self\n",
        "\n",
        "    def encode(self, text, max_length=30, pad=True):\n",
        "        \"\"\"Convert text to token indices\"\"\"\n",
        "        words = text.lower().split()[:max_length]\n",
        "        tokens = [self.word2idx.get(w, 1) for w in words]\n",
        "\n",
        "        if pad:\n",
        "            if len(tokens) < max_length:\n",
        "                tokens = tokens + [0] * (max_length - len(tokens))\n",
        "            else:\n",
        "                tokens = tokens[:max_length]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Convert token indices back to text\"\"\"\n",
        "        words = [self.idx2word.get(int(t), \"<UNK>\") for t in tokens if t > 0]\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART B: IMAGE LOADING UTILITIES\n",
        "# =========================================================\n",
        "\n",
        "def load_image_from_url(url, image_size=224, timeout=5):\n",
        "    \"\"\"Load image from URL\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_dummy_image(image_size=224):\n",
        "    \"\"\"Create dummy image if loading fails\"\"\"\n",
        "    return Image.new(\"RGB\", (image_size, image_size), color=(128, 128, 128))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART C: CUSTOM DATASET CLASS\n",
        "# =========================================================\n",
        "\n",
        "class StorySequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for visual storytelling.\n",
        "\n",
        "    Strategy: For each story with K frames,\n",
        "    - Input: frames[0:K] (all K frames)\n",
        "    - Target: frames[K-1] (last frame as target, which is frame K)\n",
        "\n",
        "    This works because VIST stories have exactly K=5 frames each.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        stories_dict,\n",
        "        tokenizer,\n",
        "        image_size=224,\n",
        "        max_text_length=30,\n",
        "        num_frames=5,\n",
        "        augment=True\n",
        "    ):\n",
        "        self.stories = stories_dict\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_size = image_size\n",
        "        self.max_text_length = max_text_length\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        # Build list of story_ids with correct number of frames\n",
        "        self.story_ids = [\n",
        "            sid for sid, frames in stories_dict.items()\n",
        "            if len(frames) >= num_frames\n",
        "        ]\n",
        "\n",
        "        print(f\"   Found {len(self.story_ids)} stories with ‚â•{num_frames} frames\")\n",
        "\n",
        "        # Image preprocessing\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Data augmentation (optional)\n",
        "        self.augment = augment\n",
        "        if augment:\n",
        "            self.augment_transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "                transforms.RandomRotation(5),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.story_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get one training sample\"\"\"\n",
        "        story_id = self.story_ids[idx]\n",
        "        frames = self.stories[story_id]\n",
        "\n",
        "        # Use first K frames as input\n",
        "        input_frames = frames[:self.num_frames]\n",
        "        # Use the LAST of the K frames as target\n",
        "        target_frame = frames[self.num_frames - 1]\n",
        "\n",
        "        # Load and process images\n",
        "        input_images = []\n",
        "        for frame in input_frames:\n",
        "            img = load_image_from_url(frame[\"url_o\"], self.image_size)\n",
        "            if img is None:\n",
        "                img = create_dummy_image(self.image_size)\n",
        "\n",
        "            # Apply augmentation to input images\n",
        "            if self.augment:\n",
        "                img = self.augment_transform(img)\n",
        "\n",
        "            img = self.transform(img)\n",
        "            input_images.append(img)\n",
        "\n",
        "        # Load target image\n",
        "        target_img = load_image_from_url(target_frame[\"url_o\"], self.image_size)\n",
        "        if target_img is None:\n",
        "            target_img = create_dummy_image(self.image_size)\n",
        "\n",
        "        target_img = self.transform(target_img)\n",
        "\n",
        "        # Stack images: shape (K, 3, H, W)\n",
        "        input_images = torch.stack(input_images, dim=0)\n",
        "\n",
        "        # Tokenize texts\n",
        "        input_texts = [frame[\"text\"] for frame in input_frames]\n",
        "        target_text = target_frame[\"text\"]\n",
        "\n",
        "        input_tokens = [self.tokenizer.encode(text, self.max_text_length) for text in input_texts]\n",
        "        input_tokens = torch.tensor(input_tokens, dtype=torch.long)\n",
        "\n",
        "        target_tokens = torch.tensor(\n",
        "            self.tokenizer.encode(target_text, self.max_text_length),\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"story_id\": story_id,\n",
        "            \"input_images\": input_images,\n",
        "            \"input_texts\": input_texts,\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"target_image\": target_img,\n",
        "            \"target_text\": target_text,\n",
        "            \"target_tokens\": target_tokens\n",
        "        }\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART D: BUILD VOCABULARY & TOKENIZER\n",
        "# =========================================================\n",
        "\n",
        "all_texts = []\n",
        "for story_id, frames in STORIES.items():\n",
        "    for frame in frames:\n",
        "        all_texts.append(frame[\"text\"])\n",
        "\n",
        "print(f\"\\nüìù Building tokenizer from {len(all_texts)} texts...\")\n",
        "tokenizer = SimpleTokenizer(vocab_size=config[\"data\"][\"vocab_size\"])\n",
        "tokenizer.build_vocab(all_texts)\n",
        "\n",
        "import pickle\n",
        "tokenizer_path = f\"{OUTPUT_DIR}/tokenizer.pkl\"\n",
        "with open(tokenizer_path, \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(f\"üíæ Tokenizer saved to: {tokenizer_path}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART E: CREATE DATASETS & DATALOADERS\n",
        "# =========================================================\n",
        "\n",
        "# Split stories into train/val/test\n",
        "story_ids = list(STORIES.keys())\n",
        "n_stories = len(story_ids)\n",
        "n_train = int(n_stories * config[\"data\"][\"train_split\"])\n",
        "n_val = int(n_stories * config[\"data\"][\"val_split\"])\n",
        "\n",
        "set_seed(SEED)\n",
        "random.shuffle(story_ids)\n",
        "\n",
        "train_story_ids = story_ids[:n_train]\n",
        "val_story_ids = story_ids[n_train:n_train + n_val]\n",
        "test_story_ids = story_ids[n_train + n_val:]\n",
        "\n",
        "train_stories = {sid: STORIES[sid] for sid in train_story_ids}\n",
        "val_stories = {sid: STORIES[sid] for sid in val_story_ids}\n",
        "test_stories = {sid: STORIES[sid] for sid in test_story_ids}\n",
        "\n",
        "print(f\"\\nüìä Data split (BEFORE subset):\")\n",
        "print(f\"   Train stories: {len(train_stories)}\")\n",
        "print(f\"   Val stories: {len(val_stories)}\")\n",
        "print(f\"   Test stories: {len(test_stories)}\")\n",
        "\n",
        "# Create full datasets\n",
        "print(f\"\\nüîß Creating full datasets...\")\n",
        "train_dataset_full = StorySequenceDataset(\n",
        "    train_stories,\n",
        "    tokenizer,\n",
        "    image_size=config[\"data\"][\"image_size\"],\n",
        "    max_text_length=config[\"data\"][\"max_text_length\"],\n",
        "    num_frames=config[\"data\"][\"num_frames\"],\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "val_dataset_full = StorySequenceDataset(\n",
        "    val_stories,\n",
        "    tokenizer,\n",
        "    image_size=config[\"data\"][\"image_size\"],\n",
        "    max_text_length=config[\"data\"][\"max_text_length\"],\n",
        "    num_frames=config[\"data\"][\"num_frames\"],\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "test_dataset_full = StorySequenceDataset(\n",
        "    test_stories,\n",
        "    tokenizer,\n",
        "    image_size=config[\"data\"][\"image_size\"],\n",
        "    max_text_length=config[\"data\"][\"max_text_length\"],\n",
        "    num_frames=config[\"data\"][\"num_frames\"],\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# ‚ö†Ô∏è  SUBSET FOR TESTING (MODIFY HERE)\n",
        "# ========================================\n",
        "\n",
        "SUBSET_SIZE = 200  # ‚Üê CHANGE THIS VALUE\n",
        "USE_SUBSET = True  # Set to False to use full dataset\n",
        "\n",
        "if USE_SUBSET:\n",
        "    print(f\"\\n‚ö†Ô∏è  USING SUBSET FOR TESTING\")\n",
        "    print(f\"   Original train: {len(train_dataset_full)}\")\n",
        "    print(f\"   Subset size: {SUBSET_SIZE}\")\n",
        "\n",
        "    # Create subsets\n",
        "    train_indices = list(range(min(SUBSET_SIZE, len(train_dataset_full))))\n",
        "    val_indices = list(range(min(SUBSET_SIZE // 5, len(val_dataset_full))))\n",
        "    test_indices = list(range(min(SUBSET_SIZE // 5, len(test_dataset_full))))\n",
        "\n",
        "    train_dataset = Subset(train_dataset_full, train_indices)\n",
        "    val_dataset = Subset(val_dataset_full, val_indices)\n",
        "    test_dataset = Subset(test_dataset_full, test_indices)\n",
        "\n",
        "    print(f\"   New train: {len(train_dataset)}\")\n",
        "    print(f\"   New val: {len(val_dataset)}\")\n",
        "    print(f\"   New test: {len(test_dataset)}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ USING FULL DATASET\")\n",
        "    train_dataset = train_dataset_full\n",
        "    val_dataset = val_dataset_full\n",
        "    test_dataset = test_dataset_full\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"training\"][\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config[\"training\"][\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config[\"training\"][\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders created:\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Val batches: {len(val_loader)}\")\n",
        "print(f\"   Test batches: {len(test_loader)}\")\n",
        "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
        "\n",
        "# Estimate training time\n",
        "est_time_per_epoch = len(train_loader) * 5 / 60\n",
        "est_total_time = est_time_per_epoch * config['training']['num_epochs']\n",
        "print(f\"\\n‚è±Ô∏è  Estimated training time:\")\n",
        "print(f\"   Per epoch: ~{est_time_per_epoch:.1f} minutes\")\n",
        "print(f\"   Total ({config['training']['num_epochs']} epochs): ~{est_total_time:.1f} minutes\")\n",
        "\n",
        "# =========================================================\n",
        "# PART F: TEST ONE BATCH\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing one batch from train_loader\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    print(f\"\\n‚úÖ Successfully loaded batch!\")\n",
        "    print(f\"üì¶ Batch keys: {list(sample_batch.keys())}\")\n",
        "    print(f\"   input_images shape: {sample_batch['input_images'].shape}\")\n",
        "    print(f\"   input_tokens shape: {sample_batch['input_tokens'].shape}\")\n",
        "    print(f\"   target_image shape: {sample_batch['target_image'].shape}\")\n",
        "    print(f\"   target_tokens shape: {sample_batch['target_tokens'].shape}\")\n",
        "\n",
        "    print(f\"\\nüìù Sample input texts (first sequence):\")\n",
        "    for i in range(config[\"data\"][\"num_frames\"]):\n",
        "        text = sample_batch['input_texts'][i]\n",
        "        print(f\"   Frame {i+1}: {text[:70]}...\")\n",
        "\n",
        "    print(f\"\\nüéØ Sample target text:\")\n",
        "    print(f\"   {sample_batch['target_text'][0][:70]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loading successful!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtBCw1Al0reu",
        "outputId": "434bc76a-3af8-47bc-f052-668a0d8768d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Visual Encoder\n",
            "================================================================================\n",
            "üîì All ResNet50 layers trainable\n",
            "‚úÖ Visual Encoder initialized\n",
            "   Output dim: 2048\n",
            "\n",
            "‚úÖ Visual Encoder Test Passed!\n",
            "   Input shape: torch.Size([2, 5, 3, 224, 224])\n",
            "   Output shape: torch.Size([2, 5, 2048])\n",
            "   Expected: (batch_size=2, K=5, feature_dim=2048)\n",
            "\n",
            "üìä Parameters:\n",
            "   Total: 23,508,032\n",
            "   Trainable: 23,508,032\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 6: Visual Encoder (CNN)\n",
        "# ============================================\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "class VisualEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN-based visual encoder using ResNet50.\n",
        "    Extracts 2048-dimensional feature vectors from images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(VisualEncoder, self).__init__()\n",
        "\n",
        "        self.feature_dim = config[\"model\"][\"visual_feature_dim\"]\n",
        "\n",
        "        # Load pre-trained ResNet50\n",
        "        resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Remove the final classification layer\n",
        "        self.backbone = nn.Sequential(*list(resnet50.children())[:-1])\n",
        "\n",
        "        # Freeze early layers if specified\n",
        "        if config[\"model\"][\"freeze_visual_encoder\"]:\n",
        "            for param in self.backbone[:-1].parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"üîí Early layers of ResNet50 frozen\")\n",
        "        else:\n",
        "            print(\"üîì All ResNet50 layers trainable\")\n",
        "\n",
        "        print(f\"‚úÖ Visual Encoder initialized\")\n",
        "        print(f\"   Output dim: {self.feature_dim}\")\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: (batch_size, K, 3, H, W) - K frames per story\n",
        "\n",
        "        Returns:\n",
        "            features: (batch_size, K, feature_dim)\n",
        "        \"\"\"\n",
        "        batch_size, K, C, H, W = images.shape\n",
        "\n",
        "        # Reshape to process all frames at once\n",
        "        images = images.view(batch_size * K, C, H, W)\n",
        "\n",
        "        # Extract features\n",
        "        features = self.backbone(images)  # (batch_size*K, feature_dim, 1, 1)\n",
        "        features = features.squeeze(-1).squeeze(-1)  # (batch_size*K, feature_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        features = features.view(batch_size, K, self.feature_dim)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE VISUAL ENCODER\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Visual Encoder\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "visual_encoder = VisualEncoder(config).to(device)\n",
        "\n",
        "# Create dummy input: (batch_size=2, K=5, 3, 224, 224)\n",
        "dummy_images = torch.randn(2, config[\"data\"][\"num_frames\"], 3, 224, 224).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    visual_features = visual_encoder(dummy_images)\n",
        "\n",
        "print(f\"\\n‚úÖ Visual Encoder Test Passed!\")\n",
        "print(f\"   Input shape: {dummy_images.shape}\")\n",
        "print(f\"   Output shape: {visual_features.shape}\")\n",
        "print(f\"   Expected: (batch_size=2, K=5, feature_dim=2048)\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in visual_encoder.parameters())\n",
        "trainable_params = sum(p.numel() for p in visual_encoder.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Parameters:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaJ2b7aW18Q7",
        "outputId": "3fa7afd1-efc8-45ed-f586-2ebd95728f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Visual Encoder with REAL BATCH from train_loader\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì¶ Real batch info:\n",
            "   Batch size: 16\n",
            "   Num frames (K): 5\n",
            "   Image shape: torch.Size([3, 224, 224])\n",
            "   Story IDs: ['11229', '17719', '15014']...\n",
            "\n",
            "üîÑ Running forward pass...\n",
            "\n",
            "‚úÖ Forward pass successful!\n",
            "   Input shape: torch.Size([16, 5, 3, 224, 224])\n",
            "   Output shape: torch.Size([16, 5, 2048])\n",
            "\n",
            "üìä Feature statistics:\n",
            "   Mean: 0.4486\n",
            "   Std: 0.3798\n",
            "   Min: 0.0000\n",
            "   Max: 4.5865\n",
            "   Contains NaN: False\n",
            "   Contains Inf: False\n",
            "\n",
            "üîç First frame features (first 10 dimensions):\n",
            "   tensor([0.7832, 1.7900, 1.0718, 0.0943, 0.7175, 1.1377, 0.5882, 0.7906, 0.3718,\n",
            "        0.3748], device='cuda:0')\n",
            "\n",
            "üìà Feature diversity (L2 distance between consecutive frames):\n",
            "   Frame 0 ‚Üí Frame 1: 20.7952\n",
            "   Frame 1 ‚Üí Frame 2: 20.0023\n",
            "   Frame 2 ‚Üí Frame 3: 17.0882\n",
            "   Frame 3 ‚Üí Frame 4: 22.3293\n",
            "\n",
            "‚úÖ Visual Encoder validation complete!\n",
            "   All checks passed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# TEST VISUAL ENCODER WITH REAL DATA\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Visual Encoder with REAL BATCH from train_loader\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get one real batch\n",
        "real_batch = next(iter(train_loader))\n",
        "real_images = real_batch['input_images'].to(device)  # (batch_size, K, 3, 224, 224)\n",
        "\n",
        "print(f\"\\nüì¶ Real batch info:\")\n",
        "print(f\"   Batch size: {real_images.shape[0]}\")\n",
        "print(f\"   Num frames (K): {real_images.shape[1]}\")\n",
        "print(f\"   Image shape: {real_images.shape[2:]}\")\n",
        "print(f\"   Story IDs: {real_batch['story_id'][:3]}...\")\n",
        "\n",
        "# Forward pass\n",
        "print(f\"\\nüîÑ Running forward pass...\")\n",
        "with torch.no_grad():\n",
        "    visual_features = visual_encoder(real_images)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "print(f\"   Input shape: {real_images.shape}\")\n",
        "print(f\"   Output shape: {visual_features.shape}\")\n",
        "\n",
        "# Check feature statistics\n",
        "print(f\"\\nüìä Feature statistics:\")\n",
        "print(f\"   Mean: {visual_features.mean().item():.4f}\")\n",
        "print(f\"   Std: {visual_features.std().item():.4f}\")\n",
        "print(f\"   Min: {visual_features.min().item():.4f}\")\n",
        "print(f\"   Max: {visual_features.max().item():.4f}\")\n",
        "\n",
        "# Check for NaN or Inf\n",
        "has_nan = torch.isnan(visual_features).any()\n",
        "has_inf = torch.isinf(visual_features).any()\n",
        "print(f\"   Contains NaN: {has_nan}\")\n",
        "print(f\"   Contains Inf: {has_inf}\")\n",
        "\n",
        "# Visualize feature for first image in batch\n",
        "print(f\"\\nüîç First frame features (first 10 dimensions):\")\n",
        "print(f\"   {visual_features[0, 0, :10]}\")\n",
        "\n",
        "# Check feature diversity across frames\n",
        "print(f\"\\nüìà Feature diversity (L2 distance between consecutive frames):\")\n",
        "for i in range(config[\"data\"][\"num_frames\"] - 1):\n",
        "    dist = torch.norm(visual_features[0, i] - visual_features[0, i+1]).item()\n",
        "    print(f\"   Frame {i} ‚Üí Frame {i+1}: {dist:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Visual Encoder validation complete!\")\n",
        "print(\"   All checks passed!\" if not (has_nan or has_inf) else \"   ‚ö†Ô∏è Warning: NaN/Inf detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w02tz5mk2r8p",
        "outputId": "10c3c167-b72d-45f6-a695-84a579fce5da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Text Encoder with REAL BATCH\n",
            "================================================================================\n",
            "‚úÖ Text Encoder initialized\n",
            "   Vocab size: 10000\n",
            "   Embedding dim: 300\n",
            "   LSTM hidden: 512\n",
            "   Bidirectional: True\n",
            "   Output dim: 1024\n",
            "\n",
            "üì¶ Text batch info:\n",
            "   Batch size: 16\n",
            "   Num captions (K): 5\n",
            "   Max length: 30\n",
            "   Sample tokens (first caption, first 10 tokens):\n",
            "   tensor([  33,   19,    6, 1449,   10, 1072, 5954,    4,    0,    0],\n",
            "       device='cuda:0')\n",
            "\n",
            "üìù Decoded first caption:\n",
            "   '[male] is a fan of fine ale .'\n",
            "\n",
            "üîÑ Running forward pass...\n",
            "\n",
            "‚úÖ Forward pass successful!\n",
            "   Input shape: torch.Size([16, 5, 30])\n",
            "   Output shape: torch.Size([16, 5, 1024])\n",
            "\n",
            "üìä Feature statistics:\n",
            "   Mean: 0.0000\n",
            "   Std: 0.0329\n",
            "   Min: -0.1707\n",
            "   Max: 0.1613\n",
            "   Contains NaN: False\n",
            "   Contains Inf: False\n",
            "\n",
            "üîç First caption features (first 10 dimensions):\n",
            "   tensor([-0.0112, -0.0313,  0.0187,  0.0033,  0.0161,  0.0432,  0.0051,  0.0319,\n",
            "        -0.0035,  0.0092], device='cuda:0')\n",
            "\n",
            "üìà Feature diversity (L2 distance between consecutive captions):\n",
            "   Caption 0 ‚Üí Caption 1: 1.1843\n",
            "   Caption 1 ‚Üí Caption 2: 0.9684\n",
            "   Caption 2 ‚Üí Caption 3: 1.0366\n",
            "   Caption 3 ‚Üí Caption 4: 1.0014\n",
            "\n",
            "üìä Parameters:\n",
            "   Total: 12,633,792\n",
            "   Trainable: 12,633,792\n",
            "\n",
            "‚úÖ Text Encoder validation complete!\n",
            "   All checks passed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 7: Text Encoder (BiLSTM)\n",
        "# ============================================\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM-based text encoder.\n",
        "    Encodes text captions into fixed-size feature vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, vocab_size):\n",
        "        super(TextEncoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = config[\"model\"][\"text_embedding_dim\"]\n",
        "        self.hidden_dim = config[\"model\"][\"text_hidden_dim\"]\n",
        "        self.num_layers = config[\"model\"][\"text_num_layers\"]\n",
        "        self.bidirectional = config[\"model\"][\"text_bidirectional\"]\n",
        "        self.output_dim = config[\"model\"][\"text_encoder_output_dim\"]\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "            padding_idx=0  # <PAD> token index\n",
        "        )\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=self.bidirectional,\n",
        "            dropout=0.3 if self.num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        # Output dimension calculation\n",
        "        lstm_output_dim = self.hidden_dim * (2 if self.bidirectional else 1)\n",
        "\n",
        "        print(f\"‚úÖ Text Encoder initialized\")\n",
        "        print(f\"   Vocab size: {vocab_size}\")\n",
        "        print(f\"   Embedding dim: {self.embedding_dim}\")\n",
        "        print(f\"   LSTM hidden: {self.hidden_dim}\")\n",
        "        print(f\"   Bidirectional: {self.bidirectional}\")\n",
        "        print(f\"   Output dim: {self.output_dim}\")\n",
        "\n",
        "    def forward(self, text_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text_tokens: (batch_size, K, max_length) - K captions per story\n",
        "\n",
        "        Returns:\n",
        "            text_features: (batch_size, K, output_dim)\n",
        "        \"\"\"\n",
        "        batch_size, K, max_length = text_tokens.shape\n",
        "\n",
        "        # Reshape to process all captions at once\n",
        "        text_tokens = text_tokens.view(batch_size * K, max_length)\n",
        "\n",
        "        # Embed tokens: (batch_size*K, max_length, embedding_dim)\n",
        "        embedded = self.embedding(text_tokens)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        # lstm_out: (batch_size*K, max_length, hidden_dim*2)\n",
        "        # hidden: (num_layers*2, batch_size*K, hidden_dim)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # Use final hidden state from both directions\n",
        "        if self.bidirectional:\n",
        "            # Concatenate forward and backward final hidden states\n",
        "            # hidden shape: (num_layers*2, batch_size*K, hidden_dim)\n",
        "            forward_hidden = hidden[-2, :, :]   # (batch_size*K, hidden_dim)\n",
        "            backward_hidden = hidden[-1, :, :]  # (batch_size*K, hidden_dim)\n",
        "            text_features = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
        "        else:\n",
        "            # Use only last layer hidden state\n",
        "            text_features = hidden[-1, :, :]\n",
        "\n",
        "        # text_features: (batch_size*K, output_dim)\n",
        "\n",
        "        # Reshape back to (batch_size, K, output_dim)\n",
        "        text_features = text_features.view(batch_size, K, self.output_dim)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE TEXT ENCODER\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Text Encoder with REAL BATCH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize text encoder\n",
        "text_encoder = TextEncoder(config, vocab_size=len(tokenizer.word2idx)).to(device)\n",
        "\n",
        "# Get real text tokens from batch\n",
        "real_text_tokens = real_batch['input_tokens'].to(device)  # (batch_size, K, max_length)\n",
        "\n",
        "print(f\"\\nüì¶ Text batch info:\")\n",
        "print(f\"   Batch size: {real_text_tokens.shape[0]}\")\n",
        "print(f\"   Num captions (K): {real_text_tokens.shape[1]}\")\n",
        "print(f\"   Max length: {real_text_tokens.shape[2]}\")\n",
        "print(f\"   Sample tokens (first caption, first 10 tokens):\")\n",
        "print(f\"   {real_text_tokens[0, 0, :10]}\")\n",
        "\n",
        "# Decode first caption to verify\n",
        "decoded_text = tokenizer.decode(real_text_tokens[0, 0].cpu().numpy())\n",
        "print(f\"\\nüìù Decoded first caption:\")\n",
        "print(f\"   '{decoded_text}'\")\n",
        "\n",
        "# Forward pass\n",
        "print(f\"\\nüîÑ Running forward pass...\")\n",
        "with torch.no_grad():\n",
        "    text_features = text_encoder(real_text_tokens)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "print(f\"   Input shape: {real_text_tokens.shape}\")\n",
        "print(f\"   Output shape: {text_features.shape}\")\n",
        "\n",
        "# Check feature statistics\n",
        "print(f\"\\nüìä Feature statistics:\")\n",
        "print(f\"   Mean: {text_features.mean().item():.4f}\")\n",
        "print(f\"   Std: {text_features.std().item():.4f}\")\n",
        "print(f\"   Min: {text_features.min().item():.4f}\")\n",
        "print(f\"   Max: {text_features.max().item():.4f}\")\n",
        "\n",
        "# Check for NaN or Inf\n",
        "has_nan = torch.isnan(text_features).any()\n",
        "has_inf = torch.isinf(text_features).any()\n",
        "print(f\"   Contains NaN: {has_nan}\")\n",
        "print(f\"   Contains Inf: {has_inf}\")\n",
        "\n",
        "# Visualize feature for first caption\n",
        "print(f\"\\nüîç First caption features (first 10 dimensions):\")\n",
        "print(f\"   {text_features[0, 0, :10]}\")\n",
        "\n",
        "# Check feature diversity across captions\n",
        "print(f\"\\nüìà Feature diversity (L2 distance between consecutive captions):\")\n",
        "for i in range(config[\"data\"][\"num_frames\"] - 1):\n",
        "    dist = torch.norm(text_features[0, i] - text_features[0, i+1]).item()\n",
        "    print(f\"   Caption {i} ‚Üí Caption {i+1}: {dist:.4f}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in text_encoder.parameters())\n",
        "trainable_params = sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Parameters:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\n‚úÖ Text Encoder validation complete!\")\n",
        "print(\"   All checks passed!\" if not (has_nan or has_inf) else \"   ‚ö†Ô∏è Warning: NaN/Inf detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L0oucS33sh2",
        "outputId": "0b0a7633-729b-43ec-d2b1-1e37a76a79c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Multimodal Fusion with REAL FEATURES\n",
            "================================================================================\n",
            "‚úÖ Multimodal Fusion initialized\n",
            "   Visual dim: 2048\n",
            "   Text dim: 1024\n",
            "   Concat dim: 3072\n",
            "   Fusion dim: 1024\n",
            "   Dropout: 0.3\n",
            "\n",
            "üì¶ Input features info:\n",
            "   Visual features shape: torch.Size([16, 5, 2048])\n",
            "   Text features shape: torch.Size([16, 5, 1024])\n",
            "\n",
            "üîÑ Running forward pass...\n",
            "\n",
            "‚úÖ Forward pass successful!\n",
            "   Output shape: torch.Size([16, 5, 1024])\n",
            "   Expected: (batch_size=16, K=5, fusion_dim=1024)\n",
            "\n",
            "üìä Feature statistics:\n",
            "   Mean: 0.0524\n",
            "   Std: 0.1006\n",
            "   Min: 0.0000\n",
            "   Max: 0.8727\n",
            "   Contains NaN: False\n",
            "   Contains Inf: False\n",
            "\n",
            "üîç First fused feature (first 10 dimensions):\n",
            "   tensor([0.0000, 0.3530, 0.0000, 0.0000, 0.0000, 0.1312, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000], device='cuda:0')\n",
            "\n",
            "üìà Feature diversity (L2 distance between consecutive frames):\n",
            "   Frame 0 ‚Üí Frame 1: 4.6685\n",
            "   Frame 1 ‚Üí Frame 2: 4.4853\n",
            "   Frame 2 ‚Üí Frame 3: 3.7174\n",
            "   Frame 3 ‚Üí Frame 4: 3.8016\n",
            "\n",
            "üìè Feature norms (first frame):\n",
            "   Visual: 28.8099\n",
            "   Text: 1.0412\n",
            "   Fused: 4.0066\n",
            "\n",
            "üìä Parameters:\n",
            "   Total: 8,391,680\n",
            "   Trainable: 8,391,680\n",
            "\n",
            "‚úÖ Multimodal Fusion validation complete!\n",
            "   All checks passed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 8: Multimodal Fusion Layer\n",
        "# ============================================\n",
        "\n",
        "class MultimodalFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Fuses visual and textual features into unified multimodal representations.\n",
        "    Uses concatenation followed by projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(MultimodalFusion, self).__init__()\n",
        "\n",
        "        self.visual_dim = config[\"model\"][\"visual_feature_dim\"]\n",
        "        self.text_dim = config[\"model\"][\"text_encoder_output_dim\"]\n",
        "        self.fusion_dim = config[\"model\"][\"fusion_dim\"]\n",
        "        self.dropout = config[\"model\"][\"fusion_dropout\"]\n",
        "\n",
        "        # Concatenated dimension\n",
        "        concat_dim = self.visual_dim + self.text_dim  # 2048 + 1024 = 3072\n",
        "\n",
        "        # Fusion layers: concat ‚Üí linear ‚Üí ReLU ‚Üí dropout ‚Üí linear\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(concat_dim, self.fusion_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.fusion_dim * 2, self.fusion_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Multimodal Fusion initialized\")\n",
        "        print(f\"   Visual dim: {self.visual_dim}\")\n",
        "        print(f\"   Text dim: {self.text_dim}\")\n",
        "        print(f\"   Concat dim: {concat_dim}\")\n",
        "        print(f\"   Fusion dim: {self.fusion_dim}\")\n",
        "        print(f\"   Dropout: {self.dropout}\")\n",
        "\n",
        "    def forward(self, visual_features, text_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            visual_features: (batch_size, K, visual_dim)\n",
        "            text_features: (batch_size, K, text_dim)\n",
        "\n",
        "        Returns:\n",
        "            fused_features: (batch_size, K, fusion_dim)\n",
        "        \"\"\"\n",
        "        batch_size, K, _ = visual_features.shape\n",
        "\n",
        "        # Concatenate visual and text features\n",
        "        # (batch_size, K, visual_dim + text_dim)\n",
        "        concat_features = torch.cat([visual_features, text_features], dim=-1)\n",
        "\n",
        "        # Reshape for processing\n",
        "        concat_features = concat_features.view(batch_size * K, -1)\n",
        "\n",
        "        # Apply fusion layers\n",
        "        fused_features = self.fusion(concat_features)\n",
        "\n",
        "        # Reshape back\n",
        "        fused_features = fused_features.view(batch_size, K, self.fusion_dim)\n",
        "\n",
        "        return fused_features\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE MULTIMODAL FUSION\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Multimodal Fusion with REAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize fusion layer\n",
        "fusion_layer = MultimodalFusion(config).to(device)\n",
        "\n",
        "print(f\"\\nüì¶ Input features info:\")\n",
        "print(f\"   Visual features shape: {visual_features.shape}\")\n",
        "print(f\"   Text features shape: {text_features.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "print(f\"\\nüîÑ Running forward pass...\")\n",
        "with torch.no_grad():\n",
        "    fused_features = fusion_layer(visual_features, text_features)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "print(f\"   Output shape: {fused_features.shape}\")\n",
        "print(f\"   Expected: (batch_size=16, K=5, fusion_dim=1024)\")\n",
        "\n",
        "# Check feature statistics\n",
        "print(f\"\\nüìä Feature statistics:\")\n",
        "print(f\"   Mean: {fused_features.mean().item():.4f}\")\n",
        "print(f\"   Std: {fused_features.std().item():.4f}\")\n",
        "print(f\"   Min: {fused_features.min().item():.4f}\")\n",
        "print(f\"   Max: {fused_features.max().item():.4f}\")\n",
        "\n",
        "# Check for NaN or Inf\n",
        "has_nan = torch.isnan(fused_features).any()\n",
        "has_inf = torch.isinf(fused_features).any()\n",
        "print(f\"   Contains NaN: {has_nan}\")\n",
        "print(f\"   Contains Inf: {has_inf}\")\n",
        "\n",
        "# Visualize fused features\n",
        "print(f\"\\nüîç First fused feature (first 10 dimensions):\")\n",
        "print(f\"   {fused_features[0, 0, :10]}\")\n",
        "\n",
        "# Check feature diversity\n",
        "print(f\"\\nüìà Feature diversity (L2 distance between consecutive frames):\")\n",
        "for i in range(config[\"data\"][\"num_frames\"] - 1):\n",
        "    dist = torch.norm(fused_features[0, i] - fused_features[0, i+1]).item()\n",
        "    print(f\"   Frame {i} ‚Üí Frame {i+1}: {dist:.4f}\")\n",
        "\n",
        "# Compare with input features (should be different)\n",
        "visual_norm = torch.norm(visual_features[0, 0]).item()\n",
        "text_norm = torch.norm(text_features[0, 0]).item()\n",
        "fused_norm = torch.norm(fused_features[0, 0]).item()\n",
        "print(f\"\\nüìè Feature norms (first frame):\")\n",
        "print(f\"   Visual: {visual_norm:.4f}\")\n",
        "print(f\"   Text: {text_norm:.4f}\")\n",
        "print(f\"   Fused: {fused_norm:.4f}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in fusion_layer.parameters())\n",
        "trainable_params = sum(p.numel() for p in fusion_layer.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Parameters:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\n‚úÖ Multimodal Fusion validation complete!\")\n",
        "print(\"   All checks passed!\" if not (has_nan or has_inf) else \"   ‚ö†Ô∏è Warning: NaN/Inf detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH0OoVYq46vI",
        "outputId": "5bc94fb8-2fb8-4e5f-e591-b4323c46f161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Tag Prediction Head (INNOVATION)\n",
            "================================================================================\n",
            "‚úÖ Tag Prediction Head initialized (INNOVATION)\n",
            "   Input dim: 1024\n",
            "   Hidden dim: 512\n",
            "   Tag vocab size: 51\n",
            "   Dropout: 0.4\n",
            "   Output: Multi-label probabilities\n",
            "\n",
            "üì¶ Input features info:\n",
            "   Fused features shape: torch.Size([16, 5, 1024])\n",
            "\n",
            "üîÑ Running forward pass...\n",
            "\n",
            "‚úÖ Forward pass successful!\n",
            "   Output shape: torch.Size([16, 5, 51])\n",
            "   Expected: (batch_size=16, K=5, tag_vocab=51)\n",
            "\n",
            "üìä Prediction statistics:\n",
            "   Mean: 0.4964\n",
            "   Std: 0.0097\n",
            "   Min: 0.4675\n",
            "   Max: 0.5315\n",
            "   Contains NaN: False\n",
            "   Contains Inf: False\n",
            "\n",
            "üè∑Ô∏è  Predicted tags for first frame (threshold=0.5):\n",
            "   Number of tags predicted: 19\n",
            "   Top predicted tags:\n",
            "      <UNK>: 0.505\n",
            "      bottle: 0.503\n",
            "      building: 0.508\n",
            "      laptop: 0.508\n",
            "      door: 0.506\n",
            "      running: 0.502\n",
            "      eating: 0.517\n",
            "      writing: 0.509\n",
            "      talking: 0.505\n",
            "      smiling: 0.500\n",
            "\n",
            "üìà Prediction diversity (L2 distance between consecutive frames):\n",
            "   Frame 0 ‚Üí Frame 1: 0.0467\n",
            "   Frame 1 ‚Üí Frame 2: 0.0559\n",
            "   Frame 2 ‚Üí Frame 3: 0.0493\n",
            "   Frame 3 ‚Üí Frame 4: 0.0460\n",
            "\n",
            "üìä Parameters:\n",
            "   Total: 669,235\n",
            "   Trainable: 669,235\n",
            "\n",
            "================================================================================\n",
            "üéØ YOUR INNOVATION VALIDATED!\n",
            "================================================================================\n",
            "‚úÖ Tag Prediction Head is working correctly!\n",
            "   This auxiliary task will improve main task performance via multi-task learning.\n",
            "   Loss weight Œª_tag = 0.3\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 9: Tag Prediction Head (INNOVATION)\n",
        "# ============================================\n",
        "\n",
        "class TagPredictionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Auxiliary task: Predicts semantic tags for each frame.\n",
        "    This is YOUR INNOVATION for multi-task learning!\n",
        "\n",
        "    Predicts tags for: objects, actions, locations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(TagPredictionHead, self).__init__()\n",
        "\n",
        "        self.fusion_dim = config[\"model\"][\"fusion_dim\"]\n",
        "        self.tag_vocab_size = config[\"model\"][\"tag_vocab_size\"]\n",
        "        self.tag_hidden_dim = config[\"model\"][\"tag_hidden_dim\"]\n",
        "        self.dropout = config[\"model\"][\"tag_dropout\"]\n",
        "\n",
        "        # Tag prediction network\n",
        "        self.tag_predictor = nn.Sequential(\n",
        "            nn.Linear(self.fusion_dim, self.tag_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.tag_hidden_dim, self.tag_hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.tag_hidden_dim // 2, self.tag_vocab_size),\n",
        "            nn.Sigmoid()  # Multi-label classification\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Tag Prediction Head initialized (INNOVATION)\")\n",
        "        print(f\"   Input dim: {self.fusion_dim}\")\n",
        "        print(f\"   Hidden dim: {self.tag_hidden_dim}\")\n",
        "        print(f\"   Tag vocab size: {self.tag_vocab_size}\")\n",
        "        print(f\"   Dropout: {self.dropout}\")\n",
        "        print(f\"   Output: Multi-label probabilities\")\n",
        "\n",
        "    def forward(self, fused_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fused_features: (batch_size, K, fusion_dim)\n",
        "\n",
        "        Returns:\n",
        "            tag_predictions: (batch_size, K, tag_vocab_size)\n",
        "                - Values in [0, 1] (sigmoid output)\n",
        "                - Each value is probability that tag is present\n",
        "        \"\"\"\n",
        "        batch_size, K, _ = fused_features.shape\n",
        "\n",
        "        # Reshape for processing\n",
        "        fused_features = fused_features.view(batch_size * K, -1)\n",
        "\n",
        "        # Predict tags\n",
        "        tag_predictions = self.tag_predictor(fused_features)\n",
        "\n",
        "        # Reshape back\n",
        "        tag_predictions = tag_predictions.view(batch_size, K, self.tag_vocab_size)\n",
        "\n",
        "        return tag_predictions\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE TAG PREDICTION HEAD\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Tag Prediction Head (INNOVATION)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize tag prediction head\n",
        "tag_predictor = TagPredictionHead(config).to(device)\n",
        "\n",
        "print(f\"\\nüì¶ Input features info:\")\n",
        "print(f\"   Fused features shape: {fused_features.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "print(f\"\\nüîÑ Running forward pass...\")\n",
        "with torch.no_grad():\n",
        "    tag_predictions = tag_predictor(fused_features)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "print(f\"   Output shape: {tag_predictions.shape}\")\n",
        "print(f\"   Expected: (batch_size=16, K=5, tag_vocab={config['model']['tag_vocab_size']})\")\n",
        "\n",
        "# Check prediction statistics\n",
        "print(f\"\\nüìä Prediction statistics:\")\n",
        "print(f\"   Mean: {tag_predictions.mean().item():.4f}\")\n",
        "print(f\"   Std: {tag_predictions.std().item():.4f}\")\n",
        "print(f\"   Min: {tag_predictions.min().item():.4f}\")\n",
        "print(f\"   Max: {tag_predictions.max().item():.4f}\")\n",
        "\n",
        "# Check for NaN or Inf\n",
        "has_nan = torch.isnan(tag_predictions).any()\n",
        "has_inf = torch.isinf(tag_predictions).any()\n",
        "print(f\"   Contains NaN: {has_nan}\")\n",
        "print(f\"   Contains Inf: {has_inf}\")\n",
        "\n",
        "# Analyze predictions (first frame)\n",
        "first_frame_preds = tag_predictions[0, 0]  # (tag_vocab_size,)\n",
        "threshold = config[\"evaluation\"][\"tag_threshold\"]  # 0.5\n",
        "\n",
        "# Get predicted tags above threshold\n",
        "predicted_tags_idx = (first_frame_preds > threshold).nonzero(as_tuple=True)[0]\n",
        "print(f\"\\nüè∑Ô∏è  Predicted tags for first frame (threshold={threshold}):\")\n",
        "print(f\"   Number of tags predicted: {len(predicted_tags_idx)}\")\n",
        "\n",
        "if len(predicted_tags_idx) > 0:\n",
        "    print(f\"   Top predicted tags:\")\n",
        "    for idx in predicted_tags_idx[:10]:  # Show max 10\n",
        "        tag_name = idx2tag[idx.item()]\n",
        "        prob = first_frame_preds[idx].item()\n",
        "        print(f\"      {tag_name}: {prob:.3f}\")\n",
        "else:\n",
        "    # Show top 5 by probability even if below threshold\n",
        "    top_probs, top_idx = torch.topk(first_frame_preds, k=5)\n",
        "    print(f\"   Top 5 tags by probability (even if < threshold):\")\n",
        "    for idx, prob in zip(top_idx, top_probs):\n",
        "        tag_name = idx2tag[idx.item()]\n",
        "        print(f\"      {tag_name}: {prob.item():.3f}\")\n",
        "\n",
        "# Check prediction diversity across frames\n",
        "print(f\"\\nüìà Prediction diversity (L2 distance between consecutive frames):\")\n",
        "for i in range(config[\"data\"][\"num_frames\"] - 1):\n",
        "    dist = torch.norm(tag_predictions[0, i] - tag_predictions[0, i+1]).item()\n",
        "    print(f\"   Frame {i} ‚Üí Frame {i+1}: {dist:.4f}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in tag_predictor.parameters())\n",
        "trainable_params = sum(p.numel() for p in tag_predictor.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Parameters:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ YOUR INNOVATION VALIDATED!\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ Tag Prediction Head is working correctly!\")\n",
        "print(\"   This auxiliary task will improve main task performance via multi-task learning.\")\n",
        "print(\"   Loss weight Œª_tag = {}\".format(config['training']['lambda_tag']))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvMPnxcS5QUA",
        "outputId": "d849449f-a104-4a86-ab47-c87af4668e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Sequence Model\n",
            "================================================================================\n",
            "‚úÖ Sequence Model initialized\n",
            "   Input dim: 1024\n",
            "   Hidden dim: 1024\n",
            "   Num layers: 2\n",
            "   Dropout: 0.3\n",
            "\n",
            "üì¶ Input features info:\n",
            "   Fused features shape: torch.Size([16, 5, 1024])\n",
            "\n",
            "üîÑ Running forward pass...\n",
            "\n",
            "‚úÖ Forward pass successful!\n",
            "   Sequence output shape: torch.Size([16, 5, 1024])\n",
            "   Expected: (batch_size=16, K=5, hidden_dim=1024)\n",
            "   Final hidden shape: torch.Size([2, 16, 1024])\n",
            "   Expected: (num_layers=2, batch_size=16, hidden_dim=1024)\n",
            "   Final cell shape: torch.Size([2, 16, 1024])\n",
            "\n",
            "üìä Sequence output statistics:\n",
            "   Mean: 0.0003\n",
            "   Std: 0.0125\n",
            "   Min: -0.0470\n",
            "   Max: 0.0568\n",
            "   Contains NaN: False\n",
            "   Contains Inf: False\n",
            "\n",
            "üìà Temporal progression (L2 distance between consecutive time steps):\n",
            "   Time 0 ‚Üí Time 1: 0.1795\n",
            "   Time 1 ‚Üí Time 2: 0.1484\n",
            "   Time 2 ‚Üí Time 3: 0.1297\n",
            "   Time 3 ‚Üí Time 4: 0.1268\n",
            "\n",
            "   First ‚Üí Last time step distance: 0.3365\n",
            "   (Should be larger, indicating temporal evolution)\n",
            "\n",
            "üîç Final hidden state (last layer, first batch, first 10 dims):\n",
            "   tensor([ 0.0100,  0.0232, -0.0054,  0.0170,  0.0028, -0.0145, -0.0026,  0.0140,\n",
            "        -0.0329,  0.0004], device='cuda:0')\n",
            "\n",
            "üìä Parameters:\n",
            "   Total: 16,793,600\n",
            "   Trainable: 16,793,600\n",
            "\n",
            "‚úÖ Sequence Model validation complete!\n",
            "   All checks passed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 10: Sequence Model (LSTM)\n",
        "# ============================================\n",
        "\n",
        "class SequenceModel(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM-based sequence model for temporal modeling.\n",
        "    Processes the K fused features to capture story progression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(SequenceModel, self).__init__()\n",
        "\n",
        "        self.input_dim = config[\"model\"][\"fusion_dim\"]\n",
        "        self.hidden_dim = config[\"model\"][\"sequence_hidden_dim\"]\n",
        "        self.num_layers = config[\"model\"][\"sequence_num_layers\"]\n",
        "        self.dropout = config[\"model\"][\"sequence_dropout\"]\n",
        "\n",
        "        # LSTM for sequence modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.input_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=self.dropout if self.num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Sequence Model initialized\")\n",
        "        print(f\"   Input dim: {self.input_dim}\")\n",
        "        print(f\"   Hidden dim: {self.hidden_dim}\")\n",
        "        print(f\"   Num layers: {self.num_layers}\")\n",
        "        print(f\"   Dropout: {self.dropout}\")\n",
        "\n",
        "    def forward(self, fused_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fused_features: (batch_size, K, fusion_dim)\n",
        "\n",
        "        Returns:\n",
        "            sequence_output: (batch_size, K, hidden_dim)\n",
        "            final_hidden: (num_layers, batch_size, hidden_dim)\n",
        "            final_cell: (num_layers, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        # LSTM forward pass\n",
        "        # sequence_output: (batch_size, K, hidden_dim)\n",
        "        # (hidden, cell): each (num_layers, batch_size, hidden_dim)\n",
        "        sequence_output, (final_hidden, final_cell) = self.lstm(fused_features)\n",
        "\n",
        "        return sequence_output, final_hidden, final_cell\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE SEQUENCE MODEL\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Sequence Model\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize sequence model\n",
        "sequence_model = SequenceModel(config).to(device)\n",
        "\n",
        "print(f\"\\nüì¶ Input features info:\")\n",
        "print(f\"   Fused features shape: {fused_features.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "print(f\"\\nüîÑ Running forward pass...\")\n",
        "with torch.no_grad():\n",
        "    sequence_output, final_hidden, final_cell = sequence_model(fused_features)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "print(f\"   Sequence output shape: {sequence_output.shape}\")\n",
        "print(f\"   Expected: (batch_size=16, K=5, hidden_dim=1024)\")\n",
        "print(f\"   Final hidden shape: {final_hidden.shape}\")\n",
        "print(f\"   Expected: (num_layers=2, batch_size=16, hidden_dim=1024)\")\n",
        "print(f\"   Final cell shape: {final_cell.shape}\")\n",
        "\n",
        "# Check output statistics\n",
        "print(f\"\\nüìä Sequence output statistics:\")\n",
        "print(f\"   Mean: {sequence_output.mean().item():.4f}\")\n",
        "print(f\"   Std: {sequence_output.std().item():.4f}\")\n",
        "print(f\"   Min: {sequence_output.min().item():.4f}\")\n",
        "print(f\"   Max: {sequence_output.max().item():.4f}\")\n",
        "\n",
        "# Check for NaN or Inf\n",
        "has_nan = torch.isnan(sequence_output).any()\n",
        "has_inf = torch.isinf(sequence_output).any()\n",
        "print(f\"   Contains NaN: {has_nan}\")\n",
        "print(f\"   Contains Inf: {has_inf}\")\n",
        "\n",
        "# Check temporal progression (output should evolve across time)\n",
        "print(f\"\\nüìà Temporal progression (L2 distance between consecutive time steps):\")\n",
        "for i in range(config[\"data\"][\"num_frames\"] - 1):\n",
        "    dist = torch.norm(sequence_output[0, i] - sequence_output[0, i+1]).item()\n",
        "    print(f\"   Time {i} ‚Üí Time {i+1}: {dist:.4f}\")\n",
        "\n",
        "# Compare first and last time step\n",
        "first_last_dist = torch.norm(sequence_output[0, 0] - sequence_output[0, -1]).item()\n",
        "print(f\"\\n   First ‚Üí Last time step distance: {first_last_dist:.4f}\")\n",
        "print(f\"   (Should be larger, indicating temporal evolution)\")\n",
        "\n",
        "# Visualize final hidden state\n",
        "print(f\"\\nüîç Final hidden state (last layer, first batch, first 10 dims):\")\n",
        "print(f\"   {final_hidden[-1, 0, :10]}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in sequence_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in sequence_model.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Parameters:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\n‚úÖ Sequence Model validation complete!\")\n",
        "print(\"   All checks passed!\" if not (has_nan or has_inf) else \"   ‚ö†Ô∏è Warning: NaN/Inf detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dacb8Ci95wWT",
        "outputId": "1c7cabcd-9ce7-47fa-b998-45b410c7e1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Attention Mechanism\n",
            "================================================================================\n",
            "‚úÖ Attention Mechanism initialized\n",
            "   Hidden dim: 1024\n",
            "   Attention dim: 1024\n",
            "   Num heads: 8\n",
            "\n",
            "üì¶ Input info:\n",
            "   Sequence output shape: torch.Size([16, 5, 1024])\n",
            "\n",
            "üîÑ Running forward pass...\n",
            "\n",
            "‚úÖ Forward pass successful!\n",
            "   Attended output shape: torch.Size([16, 5, 1024])\n",
            "   Expected: (batch_size=16, K=5, hidden_dim=1024)\n",
            "   Attention weights shape: torch.Size([16, 5, 5])\n",
            "   Expected: (batch_size=16, K=5, K=5)\n",
            "\n",
            "üìä Attended output statistics:\n",
            "   Mean: -0.0000\n",
            "   Std: 0.0047\n",
            "   Min: -0.0251\n",
            "   Max: 0.0175\n",
            "   Contains NaN: False\n",
            "   Contains Inf: False\n",
            "\n",
            "üîç Attention weights for first sample:\n",
            "   Shape: torch.Size([5, 5])\n",
            "\n",
            "   Attention matrix (how each frame attends to others):\n",
            "   Rows = Query frames, Cols = Key frames\n",
            "[[0.22222187 0.19444469 0.19444466 0.22222224 0.1666671 ]\n",
            " [0.1944467  0.1944446  0.19444492 0.22222117 0.19444376]\n",
            " [0.19444594 0.22222283 0.22222221 0.19444394 0.22222114]\n",
            " [0.19444653 0.19444391 0.16666713 0.19444466 0.22222072]\n",
            " [0.22222528 0.19444503 0.1944449  0.22222091 0.19444136]]\n",
            "\n",
            "   Row sums (should be ~1.0): tensor([1.0000, 1.0000, 1.0556, 0.9722, 1.0278], device='cuda:0')\n",
            "\n",
            "üìà Average attention received by each frame:\n",
            "   Frame 0: 0.2056\n",
            "   Frame 1: 0.2000\n",
            "   Frame 2: 0.1944\n",
            "   Frame 3: 0.2111\n",
            "   Frame 4: 0.2000\n",
            "\n",
            "   Most attended frame: Frame 3 (0.2111)\n",
            "\n",
            "üìä Parameters:\n",
            "   Total: 4,198,400\n",
            "   Trainable: 4,198,400\n",
            "\n",
            "‚úÖ Attention Mechanism validation complete!\n",
            "   All checks passed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 11: Attention Mechanism\n",
        "# ============================================\n",
        "\n",
        "class AttentionMechanism(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention mechanism to focus on relevant frames.\n",
        "    Allows model to dynamically weight importance of each frame.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(AttentionMechanism, self).__init__()\n",
        "\n",
        "        self.hidden_dim = config[\"model\"][\"sequence_hidden_dim\"]\n",
        "        self.attention_dim = config[\"model\"][\"attention_dim\"]\n",
        "        self.num_heads = config[\"model\"][\"attention_heads\"]\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.multihead_attn = nn.MultiheadAttention(\n",
        "            embed_dim=self.hidden_dim,\n",
        "            num_heads=self.num_heads,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Attention Mechanism initialized\")\n",
        "        print(f\"   Hidden dim: {self.hidden_dim}\")\n",
        "        print(f\"   Attention dim: {self.attention_dim}\")\n",
        "        print(f\"   Num heads: {self.num_heads}\")\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence_output: (batch_size, K, hidden_dim)\n",
        "\n",
        "        Returns:\n",
        "            attended_output: (batch_size, K, hidden_dim)\n",
        "            attention_weights: (batch_size, K, K)\n",
        "        \"\"\"\n",
        "        # Self-attention: each frame attends to all frames\n",
        "        # query, key, value are all sequence_output\n",
        "        attended_output, attention_weights = self.multihead_attn(\n",
        "            query=sequence_output,\n",
        "            key=sequence_output,\n",
        "            value=sequence_output,\n",
        "            need_weights=True,\n",
        "            average_attn_weights=True\n",
        "        )\n",
        "\n",
        "        return attended_output, attention_weights\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE ATTENTION MECHANISM\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Attention Mechanism\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize attention mechanism\n",
        "attention_layer = AttentionMechanism(config).to(device)\n",
        "\n",
        "print(f\"\\nüì¶ Input info:\")\n",
        "print(f\"   Sequence output shape: {sequence_output.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "print(f\"\\nüîÑ Running forward pass...\")\n",
        "with torch.no_grad():\n",
        "    attended_output, attention_weights = attention_layer(sequence_output)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "print(f\"   Attended output shape: {attended_output.shape}\")\n",
        "print(f\"   Expected: (batch_size=16, K=5, hidden_dim=1024)\")\n",
        "print(f\"   Attention weights shape: {attention_weights.shape}\")\n",
        "print(f\"   Expected: (batch_size=16, K=5, K=5)\")\n",
        "\n",
        "# Check output statistics\n",
        "print(f\"\\nüìä Attended output statistics:\")\n",
        "print(f\"   Mean: {attended_output.mean().item():.4f}\")\n",
        "print(f\"   Std: {attended_output.std().item():.4f}\")\n",
        "print(f\"   Min: {attended_output.min().item():.4f}\")\n",
        "print(f\"   Max: {attended_output.max().item():.4f}\")\n",
        "\n",
        "# Check for NaN or Inf\n",
        "has_nan = torch.isnan(attended_output).any()\n",
        "has_inf = torch.isinf(attended_output).any()\n",
        "print(f\"   Contains NaN: {has_nan}\")\n",
        "print(f\"   Contains Inf: {has_inf}\")\n",
        "\n",
        "# Analyze attention weights (first sample in batch)\n",
        "print(f\"\\nüîç Attention weights for first sample:\")\n",
        "print(f\"   Shape: {attention_weights[0].shape}\")\n",
        "print(f\"\\n   Attention matrix (how each frame attends to others):\")\n",
        "print(f\"   Rows = Query frames, Cols = Key frames\")\n",
        "print(attention_weights[0].cpu().numpy())\n",
        "\n",
        "# Check if attention weights sum to 1 (they should)\n",
        "attn_sums = attention_weights[0].sum(dim=1)\n",
        "print(f\"\\n   Row sums (should be ~1.0): {attn_sums}\")\n",
        "\n",
        "# Find which frames get most attention\n",
        "print(f\"\\nüìà Average attention received by each frame:\")\n",
        "avg_attention = attention_weights[0].mean(dim=0)\n",
        "for i, attn in enumerate(avg_attention):\n",
        "    print(f\"   Frame {i}: {attn.item():.4f}\")\n",
        "\n",
        "# Most attended frame\n",
        "most_attended = avg_attention.argmax().item()\n",
        "print(f\"\\n   Most attended frame: Frame {most_attended} ({avg_attention[most_attended].item():.4f})\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in attention_layer.parameters())\n",
        "trainable_params = sum(p.numel() for p in attention_layer.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Parameters:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\n‚úÖ Attention Mechanism validation complete!\")\n",
        "print(\"   All checks passed!\" if not (has_nan or has_inf) else \"   ‚ö†Ô∏è Warning: NaN/Inf detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ-R1DBa5--P",
        "outputId": "9983f679-7d44-452c-a936-cf6e56f4653e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Dual Decoders\n",
            "================================================================================\n",
            "‚úÖ Image Decoder initialized\n",
            "   Input dim: 1024\n",
            "   Hidden dims: [1024, 2048, 2048]\n",
            "   Output dim: 2048\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Text Decoder initialized\n",
            "   Input dim: 1024\n",
            "   Hidden dim: 1024\n",
            "   Vocab size: 10000\n",
            "   Num layers: 2\n",
            "\n",
            "üì¶ Context info:\n",
            "   Context shape: torch.Size([16, 1024])\n",
            "\n",
            "üñºÔ∏è  Testing Image Decoder...\n",
            "‚úÖ Image Decoder successful!\n",
            "   Output shape: torch.Size([16, 2048])\n",
            "   Expected: (batch_size=16, visual_dim=2048)\n",
            "   Mean: 0.0005\n",
            "   Std: 0.0152\n",
            "\n",
            "üìù Testing Text Decoder...\n",
            "‚úÖ Text Decoder successful!\n",
            "   Output shape: torch.Size([16, 30, 10000])\n",
            "   Expected: (batch_size=16, max_length=30, vocab_size=10000)\n",
            "\n",
            "üîç Sample prediction (first in batch):\n",
            "   Ground truth: he sips the ale with gusto .\n",
            "   Predicted: governor cat cat cat cat governor cat governor governor governor bundle cat cat cat cat governor cat governor cat cat cat governor governor governor governor governor governor governor cat cat\n",
            "\n",
            "üìä Parameters:\n",
            "   Image Decoder: 11,541,504\n",
            "   Text Decoder: 68,003,600\n",
            "   Total Decoders: 79,545,104\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ALL MODEL COMPONENTS VALIDATED!\n",
            "================================================================================\n",
            "üéâ Complete architecture working:\n",
            "   1. Visual Encoder (ResNet50) ‚úÖ\n",
            "   2. Text Encoder (BiLSTM) ‚úÖ\n",
            "   3. Multimodal Fusion ‚úÖ\n",
            "   4. Tag Prediction Head (YOUR INNOVATION) ‚úÖ\n",
            "   5. Sequence Model (LSTM) ‚úÖ\n",
            "   6. Attention Mechanism ‚úÖ\n",
            "   7. Dual Decoders (Image + Text) ‚úÖ\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 12: Dual Decoders (Image + Text)\n",
        "# ============================================\n",
        "\n",
        "class ImageDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decodes attended features into target image features.\n",
        "    Predicts visual features of the next frame.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(ImageDecoder, self).__init__()\n",
        "\n",
        "        self.input_dim = config[\"model\"][\"sequence_hidden_dim\"]\n",
        "        self.hidden_dims = config[\"model\"][\"image_decoder_hidden_dims\"]\n",
        "        self.output_dim = config[\"model\"][\"visual_feature_dim\"]  # 2048\n",
        "        self.dropout = config[\"model\"][\"image_decoder_dropout\"]\n",
        "\n",
        "        # Build decoder layers\n",
        "        layers = []\n",
        "        prev_dim = self.input_dim\n",
        "\n",
        "        for hidden_dim in self.hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(self.dropout)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Final layer to output dimension\n",
        "        layers.append(nn.Linear(prev_dim, self.output_dim))\n",
        "\n",
        "        self.decoder = nn.Sequential(*layers)\n",
        "\n",
        "        print(f\"‚úÖ Image Decoder initialized\")\n",
        "        print(f\"   Input dim: {self.input_dim}\")\n",
        "        print(f\"   Hidden dims: {self.hidden_dims}\")\n",
        "        print(f\"   Output dim: {self.output_dim}\")\n",
        "\n",
        "    def forward(self, attended_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            attended_features: (batch_size, hidden_dim)\n",
        "                - Use final frame's attended output\n",
        "\n",
        "        Returns:\n",
        "            predicted_image_features: (batch_size, visual_feature_dim)\n",
        "        \"\"\"\n",
        "        return self.decoder(attended_features)\n",
        "\n",
        "\n",
        "class TextDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decodes attended features into target text.\n",
        "    Generates next caption word by word.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, vocab_size):\n",
        "        super(TextDecoder, self).__init__()\n",
        "\n",
        "        self.input_dim = config[\"model\"][\"sequence_hidden_dim\"]\n",
        "        self.hidden_dim = config[\"model\"][\"text_decoder_hidden_dim\"]\n",
        "        self.num_layers = config[\"model\"][\"text_decoder_num_layers\"]\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = config[\"model\"][\"text_decoder_dropout\"]\n",
        "\n",
        "        # LSTM decoder\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.input_dim + self.vocab_size,  # context + prev word\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=self.dropout if self.num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        # Output projection to vocabulary\n",
        "        self.fc_out = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "\n",
        "        print(f\"‚úÖ Text Decoder initialized\")\n",
        "        print(f\"   Input dim: {self.input_dim}\")\n",
        "        print(f\"   Hidden dim: {self.hidden_dim}\")\n",
        "        print(f\"   Vocab size: {self.vocab_size}\")\n",
        "        print(f\"   Num layers: {self.num_layers}\")\n",
        "\n",
        "    def forward(self, context, target_tokens=None, max_length=30):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            context: (batch_size, hidden_dim) - context from attended features\n",
        "            target_tokens: (batch_size, max_length) - ground truth for teacher forcing\n",
        "            max_length: max sequence length for generation\n",
        "\n",
        "        Returns:\n",
        "            outputs: (batch_size, max_length, vocab_size) - logits\n",
        "        \"\"\"\n",
        "        batch_size = context.shape[0]\n",
        "\n",
        "        if target_tokens is not None:\n",
        "            # Teacher forcing mode (training)\n",
        "            seq_length = target_tokens.shape[1]\n",
        "            outputs = []\n",
        "\n",
        "            # Initialize hidden state with context\n",
        "            hidden = context.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "            cell = torch.zeros_like(hidden)\n",
        "\n",
        "            for t in range(seq_length):\n",
        "                # Get current token (one-hot)\n",
        "                if t == 0:\n",
        "                    # Start with <SOS> token (index 2)\n",
        "                    current_token = torch.zeros(batch_size, self.vocab_size).to(context.device)\n",
        "                    current_token[:, 2] = 1.0\n",
        "                else:\n",
        "                    # Use ground truth previous token\n",
        "                    prev_token_idx = target_tokens[:, t-1]\n",
        "                    current_token = F.one_hot(prev_token_idx, num_classes=self.vocab_size).float()\n",
        "\n",
        "                # Concatenate context and current token\n",
        "                lstm_input = torch.cat([context, current_token], dim=1).unsqueeze(1)\n",
        "\n",
        "                # LSTM step\n",
        "                lstm_out, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "                # Project to vocabulary\n",
        "                output = self.fc_out(lstm_out.squeeze(1))\n",
        "                outputs.append(output)\n",
        "\n",
        "            outputs = torch.stack(outputs, dim=1)  # (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        else:\n",
        "            # Inference mode (greedy decoding)\n",
        "            outputs = []\n",
        "            hidden = context.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "            cell = torch.zeros_like(hidden)\n",
        "\n",
        "            # Start with <SOS>\n",
        "            current_token = torch.zeros(batch_size, self.vocab_size).to(context.device)\n",
        "            current_token[:, 2] = 1.0\n",
        "\n",
        "            for t in range(max_length):\n",
        "                lstm_input = torch.cat([context, current_token], dim=1).unsqueeze(1)\n",
        "                lstm_out, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "                output = self.fc_out(lstm_out.squeeze(1))\n",
        "                outputs.append(output)\n",
        "\n",
        "                # Get predicted token for next step\n",
        "                predicted_idx = output.argmax(dim=1)\n",
        "                current_token = F.one_hot(predicted_idx, num_classes=self.vocab_size).float()\n",
        "\n",
        "            outputs = torch.stack(outputs, dim=1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE DUAL DECODERS\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Dual Decoders\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize decoders\n",
        "image_decoder = ImageDecoder(config).to(device)\n",
        "text_decoder = TextDecoder(config, vocab_size=len(tokenizer.word2idx)).to(device)\n",
        "\n",
        "# Use final frame's attended output as context\n",
        "context = attended_output[:, -1, :]  # (batch_size, hidden_dim)\n",
        "\n",
        "print(f\"\\nüì¶ Context info:\")\n",
        "print(f\"   Context shape: {context.shape}\")\n",
        "\n",
        "# --- TEST IMAGE DECODER ---\n",
        "print(f\"\\nüñºÔ∏è  Testing Image Decoder...\")\n",
        "with torch.no_grad():\n",
        "    predicted_image_features = image_decoder(context)\n",
        "\n",
        "print(f\"‚úÖ Image Decoder successful!\")\n",
        "print(f\"   Output shape: {predicted_image_features.shape}\")\n",
        "print(f\"   Expected: (batch_size=16, visual_dim=2048)\")\n",
        "print(f\"   Mean: {predicted_image_features.mean().item():.4f}\")\n",
        "print(f\"   Std: {predicted_image_features.std().item():.4f}\")\n",
        "\n",
        "# --- TEST TEXT DECODER ---\n",
        "print(f\"\\nüìù Testing Text Decoder...\")\n",
        "\n",
        "# Get target tokens from batch\n",
        "target_tokens_real = real_batch['target_tokens'].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Teacher forcing mode\n",
        "    predicted_text_logits = text_decoder(context, target_tokens=target_tokens_real)\n",
        "\n",
        "print(f\"‚úÖ Text Decoder successful!\")\n",
        "print(f\"   Output shape: {predicted_text_logits.shape}\")\n",
        "print(f\"   Expected: (batch_size=16, max_length=30, vocab_size={len(tokenizer.word2idx)})\")\n",
        "\n",
        "# Decode first prediction\n",
        "predicted_tokens = predicted_text_logits[0].argmax(dim=1).cpu().numpy()\n",
        "predicted_text = tokenizer.decode(predicted_tokens)\n",
        "ground_truth_text = tokenizer.decode(target_tokens_real[0].cpu().numpy())\n",
        "\n",
        "print(f\"\\nüîç Sample prediction (first in batch):\")\n",
        "print(f\"   Ground truth: {ground_truth_text}\")\n",
        "print(f\"   Predicted: {predicted_text}\")\n",
        "\n",
        "# Count parameters\n",
        "image_params = sum(p.numel() for p in image_decoder.parameters())\n",
        "text_params = sum(p.numel() for p in text_decoder.parameters())\n",
        "print(f\"\\nüìä Parameters:\")\n",
        "print(f\"   Image Decoder: {image_params:,}\")\n",
        "print(f\"   Text Decoder: {text_params:,}\")\n",
        "print(f\"   Total Decoders: {image_params + text_params:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ ALL MODEL COMPONENTS VALIDATED!\")\n",
        "print(\"=\"*80)\n",
        "print(\"üéâ Complete architecture working:\")\n",
        "print(\"   1. Visual Encoder (ResNet50) ‚úÖ\")\n",
        "print(\"   2. Text Encoder (BiLSTM) ‚úÖ\")\n",
        "print(\"   3. Multimodal Fusion ‚úÖ\")\n",
        "print(\"   4. Tag Prediction Head (YOUR INNOVATION) ‚úÖ\")\n",
        "print(\"   5. Sequence Model (LSTM) ‚úÖ\")\n",
        "print(\"   6. Attention Mechanism ‚úÖ\")\n",
        "print(\"   7. Dual Decoders (Image + Text) ‚úÖ\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HvwQwCTGAt9",
        "outputId": "f7ccd119-8d3f-4a55-d3b6-15fba359e63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU memory cleared\n",
            "   Available: 142.34 GB\n",
            "   Total: 150.12 GB\n",
            "\n",
            "‚úÖ Model config optimized for memory:\n",
            "   Batch size: 8\n",
            "   Text hidden: 512\n",
            "   Sequence hidden: 1024\n",
            "   Attention dim: 1024\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MEMORY CLEANUP & OPTIMIZATION\n",
        "# ============================================\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"‚úÖ GPU memory cleared\")\n",
        "print(f\"   Available: {torch.cuda.mem_get_info()[0] / 1e9:.2f} GB\")\n",
        "print(f\"   Total: {torch.cuda.mem_get_info()[1] / 1e9:.2f} GB\")\n",
        "\n",
        "# REDUCE MODEL SIZE - Smaller batch size and fewer parameters\n",
        "config['training']['batch_size'] = 8  # Reduced from 16\n",
        "config['model']['text_hidden_dim'] = 512  # Reduced from 512\n",
        "config['model']['sequence_hidden_dim'] = 1024  # Reduced from 1024\n",
        "config['model']['attention_dim'] = 1024  # Reduced from 1024\n",
        "\n",
        "print(\"\\n‚úÖ Model config optimized for memory:\")\n",
        "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"   Text hidden: {config['model']['text_hidden_dim']}\")\n",
        "print(f\"   Sequence hidden: {config['model']['sequence_hidden_dim']}\")\n",
        "print(f\"   Attention dim: {config['model']['attention_dim']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jKuQ2ny6TBn",
        "outputId": "0410774d-8180-46b6-ec28-d72fd4c7493d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ Testing Complete SrijaMultiTaskModel\n",
            "================================================================================\n",
            "üîì All ResNet50 layers trainable\n",
            "‚úÖ Visual Encoder initialized\n",
            "   Output dim: 2048\n",
            "‚úÖ Text Encoder initialized\n",
            "   Vocab size: 10000\n",
            "   Embedding dim: 300\n",
            "   LSTM hidden: 512\n",
            "   Bidirectional: True\n",
            "   Output dim: 1024\n",
            "‚úÖ Multimodal Fusion initialized\n",
            "   Visual dim: 2048\n",
            "   Text dim: 1024\n",
            "   Concat dim: 3072\n",
            "   Fusion dim: 1024\n",
            "   Dropout: 0.3\n",
            "‚úÖ Tag Prediction Head initialized (INNOVATION)\n",
            "   Input dim: 1024\n",
            "   Hidden dim: 512\n",
            "   Tag vocab size: 51\n",
            "   Dropout: 0.4\n",
            "   Output: Multi-label probabilities\n",
            "‚úÖ Sequence Model initialized\n",
            "   Input dim: 1024\n",
            "   Hidden dim: 1024\n",
            "   Num layers: 2\n",
            "   Dropout: 0.3\n",
            "‚úÖ Attention Mechanism initialized\n",
            "   Hidden dim: 1024\n",
            "   Attention dim: 1024\n",
            "   Num heads: 8\n",
            "‚úÖ Image Decoder initialized\n",
            "   Input dim: 1024\n",
            "   Hidden dims: [1024, 2048, 2048]\n",
            "   Output dim: 2048\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Text Decoder initialized\n",
            "   Input dim: 1024\n",
            "   Hidden dim: 1024\n",
            "   Vocab size: 10000\n",
            "   Num layers: 2\n",
            "\n",
            "‚úÖ SrijaMultiTaskModel initialized!\n",
            "   All 7 components integrated\n",
            "\n",
            "üì¶ Input info:\n",
            "   Input images shape: torch.Size([16, 5, 3, 224, 224])\n",
            "   Input tokens shape: torch.Size([16, 5, 30])\n",
            "   Target tokens shape: torch.Size([16, 30])\n",
            "\n",
            "üîÑ Running complete forward pass...\n",
            "\n",
            "‚úÖ Forward pass successful!\n",
            "\n",
            "üìä Output shapes:\n",
            "   predicted_image_features: torch.Size([16, 2048])\n",
            "   predicted_text_logits: torch.Size([16, 30, 10000])\n",
            "   tag_predictions: torch.Size([16, 5, 51])\n",
            "   visual_features: torch.Size([16, 5, 2048])\n",
            "   text_features: torch.Size([16, 5, 1024])\n",
            "   fused_features: torch.Size([16, 5, 1024])\n",
            "   sequence_output: torch.Size([16, 5, 1024])\n",
            "   attended_output: torch.Size([16, 5, 1024])\n",
            "   attention_weights: torch.Size([16, 5, 5])\n",
            "   context: torch.Size([16, 1024])\n",
            "\n",
            "üéØ Main Task Outputs:\n",
            "   Predicted image features:\n",
            "      Shape: torch.Size([16, 2048])\n",
            "      Expected: (16, 2048)\n",
            "      Mean: -0.0001\n",
            "      Std: 0.0148\n",
            "   Predicted text logits:\n",
            "      Shape: torch.Size([16, 30, 10000])\n",
            "      Expected: (16, 30, 10000)\n",
            "\n",
            "üè∑Ô∏è  Auxiliary Task Output (YOUR INNOVATION):\n",
            "   Tag predictions:\n",
            "      Shape: torch.Size([16, 5, 51])\n",
            "      Expected: (16, 5, 51)\n",
            "      Mean: 0.4998\n",
            "      Std: 0.0100\n",
            "\n",
            "üìä Model Parameters:\n",
            "   Total: 145,739,843\n",
            "   Trainable: 145,739,843\n",
            "\n",
            "üîß Parameters by component:\n",
            "   Visual Encoder: 23,508,032\n",
            "   Text Encoder: 12,633,792\n",
            "   Fusion: 8,391,680\n",
            "   Tag Predictor: 669,235\n",
            "   Sequence Model: 16,793,600\n",
            "   Attention: 4,198,400\n",
            "   Image Decoder: 11,541,504\n",
            "   Text Decoder: 68,003,600\n",
            "\n",
            "‚úÖ Output validation:\n",
            "   Contains NaN: False\n",
            "   Contains Inf: False\n",
            "\n",
            "üîÑ Testing backward pass...\n",
            "\n",
            "üîÑ Testing backward pass...\n",
            "   Loss value: 1.000251\n",
            "   Loss requires grad: True\n",
            "‚úÖ Gradients computed: True\n",
            "   Gradient mean: 0.000000\n",
            "   Gradient std: 0.000002\n",
            "   Gradient max: 0.000800\n",
            "   Gradient min: -0.000799\n",
            "\n",
            "================================================================================\n",
            "üéâ COMPLETE MODEL VALIDATED!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 13: Complete Model Architecture (CORRECTED)\n",
        "# ============================================\n",
        "\n",
        "class SrijaMultiTaskModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete multi-task learning model for visual storytelling.\n",
        "\n",
        "    Main tasks:\n",
        "    1. Image prediction (next frame visual features)\n",
        "    2. Text prediction (next frame caption)\n",
        "\n",
        "    Auxiliary task:\n",
        "    3. Tag prediction (semantic tags for each frame) - YOUR INNOVATION\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, vocab_size):\n",
        "        super(SrijaMultiTaskModel, self).__init__()\n",
        "\n",
        "        # Component 1: Visual Encoder\n",
        "        self.visual_encoder = VisualEncoder(config)\n",
        "\n",
        "        # Component 2: Text Encoder\n",
        "        self.text_encoder = TextEncoder(config, vocab_size)\n",
        "\n",
        "        # Component 3: Multimodal Fusion\n",
        "        self.fusion = MultimodalFusion(config)\n",
        "\n",
        "        # Component 4: Tag Prediction Head (INNOVATION)\n",
        "        self.tag_predictor = TagPredictionHead(config)\n",
        "\n",
        "        # Component 5: Sequence Model\n",
        "        self.sequence_model = SequenceModel(config)\n",
        "\n",
        "        # Component 6: Attention Mechanism\n",
        "        self.attention = AttentionMechanism(config)\n",
        "\n",
        "        # Component 7: Dual Decoders\n",
        "        self.image_decoder = ImageDecoder(config)\n",
        "        self.text_decoder = TextDecoder(config, vocab_size)\n",
        "\n",
        "        print(f\"\\n‚úÖ SrijaMultiTaskModel initialized!\")\n",
        "        print(f\"   All 7 components integrated\")\n",
        "\n",
        "    def forward(self, input_images, input_tokens, target_tokens=None):\n",
        "        \"\"\"\n",
        "        Complete forward pass through entire architecture.\n",
        "\n",
        "        Args:\n",
        "            input_images: (batch_size, K, 3, H, W)\n",
        "            input_tokens: (batch_size, K, max_length)\n",
        "            target_tokens: (batch_size, max_length) - for teacher forcing\n",
        "\n",
        "        Returns:\n",
        "            outputs: dict with predictions and auxiliary outputs\n",
        "        \"\"\"\n",
        "        batch_size = input_images.shape[0]\n",
        "\n",
        "        # ============ ENCODING PHASE ============\n",
        "\n",
        "        # 1. Visual Encoding\n",
        "        visual_features = self.visual_encoder(input_images)  # (B, K, 2048)\n",
        "\n",
        "        # 2. Text Encoding\n",
        "        text_features = self.text_encoder(input_tokens)  # (B, K, 1024)\n",
        "\n",
        "        # 3. Multimodal Fusion\n",
        "        fused_features = self.fusion(visual_features, text_features)  # (B, K, 1024)\n",
        "\n",
        "        # ============ AUXILIARY TASK: TAG PREDICTION ============\n",
        "\n",
        "        # 4. Tag Prediction (on all frames)\n",
        "        tag_predictions = self.tag_predictor(fused_features)  # (B, K, tag_vocab)\n",
        "\n",
        "        # ============ SEQUENCE MODELING PHASE ============\n",
        "\n",
        "        # 5. Sequence Model (temporal modeling)\n",
        "        sequence_output, final_hidden, final_cell = self.sequence_model(fused_features)\n",
        "        # sequence_output: (B, K, 1024)\n",
        "\n",
        "        # 6. Attention Mechanism\n",
        "        attended_output, attention_weights = self.attention(sequence_output)\n",
        "        # attended_output: (B, K, 1024)\n",
        "        # attention_weights: (B, K, K)\n",
        "\n",
        "        # ============ DECODING PHASE ============\n",
        "\n",
        "        # Get context from final frame's attended output\n",
        "        context = attended_output[:, -1, :]  # (B, 1024)\n",
        "\n",
        "        # 7a. Image Decoding\n",
        "        predicted_image_features = self.image_decoder(context)  # (B, 2048)\n",
        "\n",
        "        # 7b. Text Decoding\n",
        "        predicted_text_logits = self.text_decoder(context, target_tokens=target_tokens)\n",
        "        # (B, max_length, vocab_size)\n",
        "\n",
        "        # ============ RETURN OUTPUTS ============\n",
        "\n",
        "        outputs = {\n",
        "            # Main task outputs\n",
        "            'predicted_image_features': predicted_image_features,\n",
        "            'predicted_text_logits': predicted_text_logits,\n",
        "\n",
        "            # Auxiliary task output (YOUR INNOVATION)\n",
        "            'tag_predictions': tag_predictions,\n",
        "\n",
        "            # Intermediate representations (for analysis/visualization)\n",
        "            'visual_features': visual_features,\n",
        "            'text_features': text_features,\n",
        "            'fused_features': fused_features,\n",
        "            'sequence_output': sequence_output,\n",
        "            'attended_output': attended_output,\n",
        "            'attention_weights': attention_weights,\n",
        "            'context': context,\n",
        "        }\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TEST THE COMPLETE MODEL\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ Testing Complete SrijaMultiTaskModel\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize complete model\n",
        "model = SrijaMultiTaskModel(config, vocab_size=len(tokenizer.word2idx)).to(device)\n",
        "\n",
        "print(f\"\\nüì¶ Input info:\")\n",
        "print(f\"   Input images shape: {real_images.shape}\")\n",
        "print(f\"   Input tokens shape: {real_text_tokens.shape}\")\n",
        "print(f\"   Target tokens shape: {target_tokens_real.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "print(f\"\\nüîÑ Running complete forward pass...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(real_images, real_text_tokens, target_tokens=target_tokens_real)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass successful!\")\n",
        "\n",
        "# Check all outputs\n",
        "print(f\"\\nüìä Output shapes:\")\n",
        "for key, value in outputs.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"   {key}: {value.shape}\")\n",
        "    else:\n",
        "        print(f\"   {key}: {type(value).__name__}\")\n",
        "\n",
        "# Validate main outputs\n",
        "print(f\"\\nüéØ Main Task Outputs:\")\n",
        "print(f\"   Predicted image features:\")\n",
        "print(f\"      Shape: {outputs['predicted_image_features'].shape}\")\n",
        "print(f\"      Expected: (16, 2048)\")\n",
        "print(f\"      Mean: {outputs['predicted_image_features'].mean().item():.4f}\")\n",
        "print(f\"      Std: {outputs['predicted_image_features'].std().item():.4f}\")\n",
        "\n",
        "print(f\"   Predicted text logits:\")\n",
        "print(f\"      Shape: {outputs['predicted_text_logits'].shape}\")\n",
        "print(f\"      Expected: (16, 30, 10000)\")\n",
        "\n",
        "# Validate auxiliary output\n",
        "print(f\"\\nüè∑Ô∏è  Auxiliary Task Output (YOUR INNOVATION):\")\n",
        "print(f\"   Tag predictions:\")\n",
        "print(f\"      Shape: {outputs['tag_predictions'].shape}\")\n",
        "print(f\"      Expected: (16, 5, 51)\")\n",
        "print(f\"      Mean: {outputs['tag_predictions'].mean().item():.4f}\")\n",
        "print(f\"      Std: {outputs['tag_predictions'].std().item():.4f}\")\n",
        "\n",
        "# Count total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìä Model Parameters:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")\n",
        "\n",
        "# Breakdown by component\n",
        "components = {\n",
        "    'Visual Encoder': model.visual_encoder,\n",
        "    'Text Encoder': model.text_encoder,\n",
        "    'Fusion': model.fusion,\n",
        "    'Tag Predictor': model.tag_predictor,\n",
        "    'Sequence Model': model.sequence_model,\n",
        "    'Attention': model.attention,\n",
        "    'Image Decoder': model.image_decoder,\n",
        "    'Text Decoder': model.text_decoder,\n",
        "}\n",
        "\n",
        "print(f\"\\nüîß Parameters by component:\")\n",
        "for name, component in components.items():\n",
        "    comp_params = sum(p.numel() for p in component.parameters())\n",
        "    print(f\"   {name}: {comp_params:,}\")\n",
        "\n",
        "# Check for NaN/Inf in all outputs\n",
        "has_nan = any(torch.isnan(v).any() for v in outputs.values() if isinstance(v, torch.Tensor))\n",
        "has_inf = any(torch.isinf(v).any() for v in outputs.values() if isinstance(v, torch.Tensor))\n",
        "\n",
        "print(f\"\\n‚úÖ Output validation:\")\n",
        "print(f\"   Contains NaN: {has_nan}\")\n",
        "print(f\"   Contains Inf: {has_inf}\")\n",
        "\n",
        "# ============================================\n",
        "# FIX: Testing backward pass (CORRECTED)\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nüîÑ Testing backward pass...\")\n",
        "# ============================================\n",
        "# Testing backward pass (CORRECTED)\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nüîÑ Testing backward pass...\")\n",
        "model.train()\n",
        "\n",
        "# Re-run forward with gradient tracking\n",
        "outputs = model(real_images, real_text_tokens, target_tokens=target_tokens_real)\n",
        "\n",
        "# Get actual batch size from outputs\n",
        "batch_size = outputs['predicted_image_features'].shape[0]\n",
        "\n",
        "# Create dummy target with CORRECT batch size\n",
        "image_features_target = torch.randn(batch_size, 2048).to(device)\n",
        "\n",
        "# MSE Loss for image prediction\n",
        "loss = torch.nn.functional.mse_loss(outputs['predicted_image_features'], image_features_target)\n",
        "\n",
        "print(f\"   Loss value: {loss.item():.6f}\")\n",
        "print(f\"   Loss requires grad: {loss.requires_grad}\")\n",
        "\n",
        "# Backward\n",
        "loss.backward()\n",
        "\n",
        "# Check if gradients are computed\n",
        "has_gradients = any(p.grad is not None for p in model.parameters() if p.requires_grad)\n",
        "print(f\"‚úÖ Gradients computed: {has_gradients}\")\n",
        "\n",
        "if has_gradients:\n",
        "    all_grads = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None])\n",
        "    print(f\"   Gradient mean: {all_grads.mean().item():.6f}\")\n",
        "    print(f\"   Gradient std: {all_grads.std().item():.6f}\")\n",
        "    print(f\"   Gradient max: {all_grads.max().item():.6f}\")\n",
        "    print(f\"   Gradient min: {all_grads.min().item():.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ COMPLETE MODEL VALIDATED!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8apIFXy6hSo",
        "outputId": "128e1118-597c-40df-b041-f5a0673552a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ INITIALIZING TRAINING\n",
            "================================================================================\n",
            "‚úÖ MultiTaskLoss initialized\n",
            "   Œª_image: 1.0\n",
            "   Œª_text: 1.0\n",
            "   Œª_tag: 0.3 (AUXILIARY)\n",
            "‚úÖ Optimizer: ADAM\n",
            "   Learning rate: 0.0001\n",
            "   Weight decay: 1e-05\n",
            "‚úÖ Scheduler: ReduceLROnPlateau\n",
            "   Factor: 0.5\n",
            "   Patience: 5\n",
            "‚úÖ Early Stopping\n",
            "   Patience: 10\n",
            "\n",
            "‚úÖ Training setup complete!\n",
            "   Epochs: 10\n",
            "   Batch size: 8\n",
            "   Train batches: 13\n",
            "   Val batches: 3\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 14: Training Loop\n",
        "# ============================================\n",
        "\n",
        "# =========================================================\n",
        "# PART A: LOSS FUNCTIONS\n",
        "# =========================================================\n",
        "\n",
        "class MultiTaskLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined loss for multi-task learning:\n",
        "    1. Image prediction loss (MSE)\n",
        "    2. Text prediction loss (CrossEntropy)\n",
        "    3. Tag prediction loss (BCEWithLogits) - YOUR INNOVATION\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(MultiTaskLoss, self).__init__()\n",
        "\n",
        "        self.lambda_image = config['training']['lambda_image']\n",
        "        self.lambda_text = config['training']['lambda_text']\n",
        "        self.lambda_tag = config['training']['lambda_tag']\n",
        "\n",
        "        # Loss functions\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)  # Ignore <PAD> token\n",
        "        self.bce_loss = nn.BCELoss()  # For tag prediction\n",
        "\n",
        "        print(f\"‚úÖ MultiTaskLoss initialized\")\n",
        "        print(f\"   Œª_image: {self.lambda_image}\")\n",
        "        print(f\"   Œª_text: {self.lambda_text}\")\n",
        "        print(f\"   Œª_tag: {self.lambda_tag} (AUXILIARY)\")\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            outputs: dict from model with predictions\n",
        "            targets: dict with ground truth\n",
        "                - 'image_features': (B, 2048)\n",
        "                - 'text_tokens': (B, max_length)\n",
        "                - 'tag_labels': (B, K, tag_vocab) [optional]\n",
        "\n",
        "        Returns:\n",
        "            loss_dict: dict with individual and total losses\n",
        "        \"\"\"\n",
        "\n",
        "        # ========== MAIN TASK 1: IMAGE PREDICTION ==========\n",
        "        image_loss = self.mse_loss(\n",
        "            outputs['predicted_image_features'],\n",
        "            targets['image_features']\n",
        "        )\n",
        "\n",
        "        # ========== MAIN TASK 2: TEXT PREDICTION ==========\n",
        "        # Reshape for CrossEntropyLoss\n",
        "        pred_text = outputs['predicted_text_logits']  # (B, max_length, vocab_size)\n",
        "        true_text = targets['text_tokens']  # (B, max_length)\n",
        "\n",
        "        # Flatten for loss computation\n",
        "        pred_text_flat = pred_text.view(-1, pred_text.shape[-1])\n",
        "        true_text_flat = true_text.view(-1)\n",
        "\n",
        "        text_loss = self.ce_loss(pred_text_flat, true_text_flat)\n",
        "\n",
        "        # ========== AUXILIARY TASK: TAG PREDICTION (YOUR INNOVATION) ==========\n",
        "        tag_loss = 0.0\n",
        "        if 'tag_labels' in targets and targets['tag_labels'] is not None:\n",
        "            # Tag labels: (B, K, tag_vocab) - multi-label\n",
        "            tag_preds = outputs['tag_predictions']  # (B, K, tag_vocab)\n",
        "            tag_labels = targets['tag_labels']  # (B, K, tag_vocab)\n",
        "\n",
        "            # Flatten for loss computation\n",
        "            tag_preds_flat = tag_preds.view(-1, tag_preds.shape[-1])\n",
        "            tag_labels_flat = tag_labels.view(-1, tag_labels.shape[-1])\n",
        "\n",
        "            tag_loss = self.bce_loss(tag_preds_flat, tag_labels_flat)\n",
        "\n",
        "        # ========== COMBINED LOSS ==========\n",
        "        total_loss = (\n",
        "            self.lambda_image * image_loss +\n",
        "            self.lambda_text * text_loss +\n",
        "            self.lambda_tag * tag_loss\n",
        "        )\n",
        "\n",
        "        loss_dict = {\n",
        "            'total_loss': total_loss,\n",
        "            'image_loss': image_loss,\n",
        "            'text_loss': text_loss,\n",
        "            'tag_loss': tag_loss,\n",
        "        }\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART B: TRAINING UTILITIES\n",
        "# =========================================================\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_path):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"   üíæ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    \"\"\"Load model checkpoint\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return epoch, loss\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART C: TRAINING LOOP\n",
        "# =========================================================\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, config):\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    losses_log = {'image': 0.0, 'text': 0.0, 'tag': 0.0}\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Move batch to device\n",
        "        input_images = batch['input_images'].to(device)\n",
        "        input_tokens = batch['input_tokens'].to(device)\n",
        "        target_tokens = batch['target_tokens'].to(device)\n",
        "        target_image = batch['target_image'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_images, input_tokens, target_tokens=target_tokens)\n",
        "\n",
        "        # Prepare targets\n",
        "        targets = {\n",
        "            'image_features': target_image,\n",
        "            'text_tokens': target_tokens,\n",
        "            'tag_labels': None,  # We'll add real labels in future\n",
        "        }\n",
        "\n",
        "        # Compute loss\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        loss = loss_dict['total_loss']\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(),\n",
        "            config['training']['grad_clip_norm']\n",
        "        )\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging\n",
        "        total_loss += loss.item()\n",
        "        losses_log['image'] += loss_dict['image_loss'].item()\n",
        "        losses_log['text'] += loss_dict['text_loss'].item()\n",
        "        losses_log['tag'] += loss_dict['tag_loss'].item() if isinstance(loss_dict['tag_loss'], torch.Tensor) else 0.0\n",
        "        num_batches += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'avg_loss': f\"{total_loss / num_batches:.4f}\"\n",
        "        })\n",
        "\n",
        "        # Log every N steps\n",
        "        if (batch_idx + 1) % config['logging']['log_every_n_steps'] == 0:\n",
        "            print(f\"\\n   Batch {batch_idx + 1}/{len(train_loader)}\")\n",
        "            print(f\"   Loss: {loss.item():.6f}\")\n",
        "            print(f\"   Image Loss: {loss_dict['image_loss'].item():.6f}\")\n",
        "            print(f\"   Text Loss: {loss_dict['text_loss'].item():.6f}\")\n",
        "            print(f\"   Tag Loss: {loss_dict['tag_loss'].item():.6f}\")\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_losses = {k: v / num_batches for k, v in losses_log.items()}\n",
        "\n",
        "    return avg_loss, avg_losses\n",
        "\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate one epoch\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            input_images = batch['input_images'].to(device)\n",
        "            input_tokens = batch['input_tokens'].to(device)\n",
        "            target_tokens = batch['target_tokens'].to(device)\n",
        "            target_image = batch['target_image'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_images, input_tokens, target_tokens=target_tokens)\n",
        "\n",
        "            # Prepare targets\n",
        "            targets = {\n",
        "                'image_features': target_image,\n",
        "                'text_tokens': target_tokens,\n",
        "                'tag_labels': None,\n",
        "            }\n",
        "\n",
        "            # Compute loss\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "            loss = loss_dict['total_loss']\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# PART D: INITIALIZE TRAINING\n",
        "# =========================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ INITIALIZING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize loss function\n",
        "criterion = MultiTaskLoss(config).to(device)\n",
        "\n",
        "# Initialize optimizer\n",
        "if config['training']['optimizer'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config['training']['learning_rate'],\n",
        "        betas=config['training']['betas'],\n",
        "        eps=config['training']['eps'],\n",
        "        weight_decay=config['training']['weight_decay']\n",
        "    )\n",
        "elif config['training']['optimizer'] == 'adamw':\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config['training']['learning_rate'],\n",
        "        weight_decay=config['training']['weight_decay']\n",
        "    )\n",
        "else:\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=config['training']['learning_rate'],\n",
        "        weight_decay=config['training']['weight_decay']\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Optimizer: {config['training']['optimizer'].upper()}\")\n",
        "print(f\"   Learning rate: {config['training']['learning_rate']}\")\n",
        "print(f\"   Weight decay: {config['training']['weight_decay']}\")\n",
        "\n",
        "# Initialize scheduler (FIXED)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=config['training']['scheduler_factor'],\n",
        "    patience=config['training']['scheduler_patience'],\n",
        "    min_lr=config['training']['scheduler_min_lr']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Scheduler: ReduceLROnPlateau\")\n",
        "print(f\"   Factor: {config['training']['scheduler_factor']}\")\n",
        "print(f\"   Patience: {config['training']['scheduler_patience']}\")\n",
        "\n",
        "# Initialize early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=config['training']['early_stopping_patience'],\n",
        "    min_delta=1e-4\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Early Stopping\")\n",
        "print(f\"   Patience: {config['training']['early_stopping_patience']}\")\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_losses': {'image': [], 'text': [], 'tag': []},\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Training setup complete!\")\n",
        "print(f\"   Epochs: {config['training']['num_epochs']}\")\n",
        "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA-BVBBM7gSh",
        "outputId": "8b61cd53-15d1-47d4-8df3-e429722a86bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training functions corrected!\n",
            "   Now extracting target image features correctly\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 14 (CORRECTED): Training Loop - Fix targets\n",
        "# ============================================\n",
        "\n",
        "# =========================================================\n",
        "# PART C: TRAINING LOOP (CORRECTED)\n",
        "# =========================================================\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, config):\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    losses_log = {'image': 0.0, 'text': 0.0, 'tag': 0.0}\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Move batch to device\n",
        "        input_images = batch['input_images'].to(device)\n",
        "        input_tokens = batch['input_tokens'].to(device)\n",
        "        target_tokens = batch['target_tokens'].to(device)\n",
        "        target_image = batch['target_image'].to(device)  # (B, 3, 224, 224)\n",
        "\n",
        "        # Extract target image features using visual encoder\n",
        "        with torch.no_grad():\n",
        "            # Add dummy K dimension and extract features\n",
        "            target_image_exp = target_image.unsqueeze(1)  # (B, 1, 3, 224, 224)\n",
        "            target_image_features = model.visual_encoder(target_image_exp)  # (B, 1, 2048)\n",
        "            target_image_features = target_image_features.squeeze(1)  # (B, 2048)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_images, input_tokens, target_tokens=target_tokens)\n",
        "\n",
        "        # Prepare targets\n",
        "        targets = {\n",
        "            'image_features': target_image_features,  # (B, 2048) - FIXED!\n",
        "            'text_tokens': target_tokens,\n",
        "            'tag_labels': None,  # We'll add real labels in future\n",
        "        }\n",
        "\n",
        "        # Compute loss\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        loss = loss_dict['total_loss']\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(),\n",
        "            config['training']['grad_clip_norm']\n",
        "        )\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging\n",
        "        total_loss += loss.item()\n",
        "        losses_log['image'] += loss_dict['image_loss'].item()\n",
        "        losses_log['text'] += loss_dict['text_loss'].item()\n",
        "        losses_log['tag'] += loss_dict['tag_loss'].item() if isinstance(loss_dict['tag_loss'], torch.Tensor) else 0.0\n",
        "        num_batches += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'avg_loss': f\"{total_loss / num_batches:.4f}\"\n",
        "        })\n",
        "\n",
        "        # Log every N steps\n",
        "        if (batch_idx + 1) % config['logging']['log_every_n_steps'] == 0:\n",
        "            print(f\"\\n   Batch {batch_idx + 1}/{len(train_loader)}\")\n",
        "            print(f\"   Loss: {loss.item():.6f}\")\n",
        "            print(f\"   Image Loss: {loss_dict['image_loss'].item():.6f}\")\n",
        "            print(f\"   Text Loss: {loss_dict['text_loss'].item():.6f}\")\n",
        "            print(f\"   Tag Loss: {loss_dict['tag_loss']:.6f}\" if isinstance(loss_dict['tag_loss'], float) else f\"   Tag Loss: {loss_dict['tag_loss'].item():.6f}\")\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_losses = {k: v / num_batches for k, v in losses_log.items()}\n",
        "\n",
        "    return avg_loss, avg_losses\n",
        "\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate one epoch\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            input_images = batch['input_images'].to(device)\n",
        "            input_tokens = batch['input_tokens'].to(device)\n",
        "            target_tokens = batch['target_tokens'].to(device)\n",
        "            target_image = batch['target_image'].to(device)\n",
        "\n",
        "            # Extract target image features\n",
        "            target_image_exp = target_image.unsqueeze(1)\n",
        "            target_image_features = model.visual_encoder(target_image_exp).squeeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_images, input_tokens, target_tokens=target_tokens)\n",
        "\n",
        "            # Prepare targets\n",
        "            targets = {\n",
        "                'image_features': target_image_features,  # FIXED!\n",
        "                'text_tokens': target_tokens,\n",
        "                'tag_labels': None,\n",
        "            }\n",
        "\n",
        "            # Compute loss\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "            loss = loss_dict['total_loss']\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Training functions corrected!\")\n",
        "print(\"   Now extracting target image features correctly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUSQyNM1ICE6",
        "outputId": "858bc690-da70-45f9-cc82-7520b8d51eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üéØ STARTING TRAINING\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 1/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 9.418037\n",
            "   Image Loss: 0.320715\n",
            "   Text Loss: 9.097323\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 8.668109\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "   üíæ Checkpoint saved: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/best_model.pth\n",
            "   üèÜ New best model! Val Loss: 8.668109\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 3.08 minutes\n",
            "   Best Val Loss so far: 8.668109\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 2/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 7.685510\n",
            "   Image Loss: 0.250220\n",
            "   Text Loss: 7.435290\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 7.204355\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "   üíæ Checkpoint saved: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/best_model.pth\n",
            "   üèÜ New best model! Val Loss: 7.204355\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 2.87 minutes\n",
            "   Best Val Loss so far: 7.204355\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 3/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 6.321478\n",
            "   Image Loss: 0.197729\n",
            "   Text Loss: 6.123749\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 6.756166\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "   üíæ Checkpoint saved: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/best_model.pth\n",
            "   üèÜ New best model! Val Loss: 6.756166\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 2.92 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 4/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 5.831999\n",
            "   Image Loss: 0.196808\n",
            "   Text Loss: 5.635192\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 6.895139\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 2.61 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 5/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 5.786852\n",
            "   Image Loss: 0.275922\n",
            "   Text Loss: 5.510930\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 56.691879\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "   üíæ Checkpoint saved: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/checkpoint_epoch_5.pth\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 3.01 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 6/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 5.837955\n",
            "   Image Loss: 0.341990\n",
            "   Text Loss: 5.495965\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 605.174486\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 2.85 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 7/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 5.667076\n",
            "   Image Loss: 0.187027\n",
            "   Text Loss: 5.480049\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 8.268514\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 2.91 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 8/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 5.597560\n",
            "   Image Loss: 0.167448\n",
            "   Text Loss: 5.430112\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 232.652333\n",
            "\n",
            "üìâ Learning Rate: 1.00e-04\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 3.11 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 9/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 5.619832\n",
            "   Image Loss: 0.192063\n",
            "   Text Loss: 5.427769\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 7.087325\n",
            "\n",
            "üìâ Learning Rate: 5.00e-05\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 3.02 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üìÖ EPOCH 10/10\n",
            "================================================================================\n",
            "\n",
            "üèãÔ∏è  Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete\n",
            "   Total Loss: 5.493834\n",
            "   Image Loss: 0.105969\n",
            "   Text Loss: 5.387865\n",
            "   Tag Loss: 0.000000\n",
            "\n",
            "üîç Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Validation Complete\n",
            "   Val Loss: 6.952913\n",
            "\n",
            "üìâ Learning Rate: 5.00e-05\n",
            "   üíæ Checkpoint saved: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/checkpoint_epoch_10.pth\n",
            "\n",
            "‚è±Ô∏è  Epoch time: 2.97 minutes\n",
            "   Best Val Loss so far: 6.756166\n",
            "\n",
            "================================================================================\n",
            "üéâ TRAINING COMPLETE!\n",
            "================================================================================\n",
            "   Total time: 0.49 hours\n",
            "   Best Val Loss: 6.756166\n",
            "   Final Train Loss: 5.493834\n",
            "   Final Val Loss: 6.952913\n",
            "================================================================================\n",
            "   üíæ Checkpoint saved: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/final_model.pth\n",
            "\n",
            "üíæ Training history saved to: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/results/training_history.json\n",
            "\n",
            "‚úÖ Ready for evaluation and visualization!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CELL 15: Main Training Loop\n",
        "# ============================================\n",
        "\n",
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéØ STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config['training']['num_epochs']):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üìÖ EPOCH {epoch + 1}/{config['training']['num_epochs']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # ========== TRAINING PHASE ==========\n",
        "    print(f\"\\nüèãÔ∏è  Training...\")\n",
        "    train_loss, train_losses = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device, config\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ Training Complete\")\n",
        "    print(f\"   Total Loss: {train_loss:.6f}\")\n",
        "    print(f\"   Image Loss: {train_losses['image']:.6f}\")\n",
        "    print(f\"   Text Loss: {train_losses['text']:.6f}\")\n",
        "    print(f\"   Tag Loss: {train_losses['tag']:.6f}\")\n",
        "\n",
        "    # ========== VALIDATION PHASE ==========\n",
        "    print(f\"\\nüîç Validating...\")\n",
        "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\n‚úÖ Validation Complete\")\n",
        "    print(f\"   Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    # ========== LOGGING ==========\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_losses']['image'].append(train_losses['image'])\n",
        "    history['train_losses']['text'].append(train_losses['text'])\n",
        "    history['train_losses']['tag'].append(train_losses['tag'])\n",
        "\n",
        "    # ========== LEARNING RATE SCHEDULING ==========\n",
        "    scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"\\nüìâ Learning Rate: {current_lr:.2e}\")\n",
        "\n",
        "    # ========== CHECKPOINTING ==========\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        checkpoint_path = f\"{CHECKPOINT_DIR}/best_model.pth\"\n",
        "        save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path)\n",
        "        print(f\"   üèÜ New best model! Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    # Save periodic checkpoint\n",
        "    if (epoch + 1) % config['training']['save_every_n_epochs'] == 0:\n",
        "        checkpoint_path = f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pth\"\n",
        "        save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path)\n",
        "\n",
        "    # ========== EARLY STOPPING ==========\n",
        "    if config['training']['early_stopping']:\n",
        "        if early_stopping(val_loss):\n",
        "            print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch + 1}\")\n",
        "            print(f\"   Best Val Loss: {best_val_loss:.6f}\")\n",
        "            break\n",
        "\n",
        "    # ========== EPOCH SUMMARY ==========\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    print(f\"\\n‚è±Ô∏è  Epoch time: {epoch_time/60:.2f} minutes\")\n",
        "    print(f\"   Best Val Loss so far: {best_val_loss:.6f}\")\n",
        "\n",
        "# ========== TRAINING COMPLETE ==========\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"   Total time: {total_time/3600:.2f} hours\")\n",
        "print(f\"   Best Val Loss: {best_val_loss:.6f}\")\n",
        "print(f\"   Final Train Loss: {history['train_loss'][-1]:.6f}\")\n",
        "print(f\"   Final Val Loss: {history['val_loss'][-1]:.6f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ========== SAVE FINAL MODEL ==========\n",
        "final_checkpoint_path = f\"{CHECKPOINT_DIR}/final_model.pth\"\n",
        "save_checkpoint(model, optimizer, epoch, val_loss, final_checkpoint_path)\n",
        "\n",
        "# ========== SAVE TRAINING HISTORY ==========\n",
        "import json\n",
        "history_path = f\"{RESULTS_DIR}/training_history.json\"\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "print(f\"\\nüíæ Training history saved to: {history_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Ready for evaluation and visualization!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ZkbSfv4eICrh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/best_model.pth\n",
            "Model loaded and set to eval().\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "best_ckpt_path = os.path.join(CHECKPOINT_DIR, \"/teamspace/studios/this_studio/vist_kaggle/srija_outputs/checkpoints/best_model.pth\")\n",
        "final_ckpt_path = os.path.join(CHECKPOINT_DIR, \"vist_kaggle/srija_outputs/checkpoints/final_model.pth\")\n",
        "\n",
        "ckpt_path = best_ckpt_path if os.path.exists(best_ckpt_path) else final_ckpt_path\n",
        "print(\"Loading checkpoint:\", ckpt_path)\n",
        "\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "# Most notebooks save either:\n",
        "# 1) ckpt[\"model_state_dict\"]  OR  2) ckpt[\"model\"]\n",
        "if \"model_state_dict\" in ckpt:\n",
        "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "elif \"model\" in ckpt:\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "else:\n",
        "    # fallback: assume entire dict is a state_dict\n",
        "    model.load_state_dict(ckpt)\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded and set to eval().\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading: /teamspace/studios/this_studio/vist_kaggle/srija_outputs/results/training_history.json\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "history_path = os.path.join(RESULTS_DIR, \"training_history.json\")\n",
        "print(\"Reading:\", history_path)\n",
        "\n",
        "with open(history_path, \"r\") as f:\n",
        "    hist = json.load(f)\n",
        "\n",
        "# Hist can be either dict-of-lists or list-of-epoch-dicts.\n",
        "# Try to normalize.\n",
        "if isinstance(hist, list):\n",
        "    epochs = [e.get(\"epoch\", i+1) for i, e in enumerate(hist)]\n",
        "    train_loss = [e.get(\"train_loss\") for e in hist]\n",
        "    val_loss = [e.get(\"val_loss\") for e in hist]\n",
        "else:\n",
        "    epochs = list(range(1, len(hist.get(\"train_loss\", [])) + 1))\n",
        "    train_loss = hist.get(\"train_loss\", [])\n",
        "    val_loss = hist.get(\"val_loss\", [])\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(epochs, train_loss, label=\"Train loss\")\n",
        "plt.plot(epochs, val_loss, label=\"Val loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Curve\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
